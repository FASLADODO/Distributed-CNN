{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/Jinchao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(64, 3), filters=512, kernel_size=1)`\n",
      "/Users/Jinchao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:94: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 64, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              67110912  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 99)                101475    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 99)                0         \n",
      "=================================================================\n",
      "Total params: 69,312,611\n",
      "Trainable params: 69,312,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 891 samples, validate on 99 samples\n",
      "Epoch 1/15\n",
      "891/891 [==============================] - 51s 57ms/step - loss: 3.6831 - acc: 0.2469 - val_loss: 1.5827 - val_acc: 0.5960\n",
      "Epoch 2/15\n",
      "891/891 [==============================] - 51s 58ms/step - loss: 0.6870 - acc: 0.8215 - val_loss: 0.5335 - val_acc: 0.8384\n",
      "Epoch 3/15\n",
      "891/891 [==============================] - 41s 46ms/step - loss: 0.2040 - acc: 0.9551 - val_loss: 0.1749 - val_acc: 0.9798\n",
      "Epoch 4/15\n",
      "891/891 [==============================] - 43s 49ms/step - loss: 0.1144 - acc: 0.9708 - val_loss: 0.1400 - val_acc: 0.9697\n",
      "Epoch 5/15\n",
      "891/891 [==============================] - 39s 44ms/step - loss: 0.0331 - acc: 0.9933 - val_loss: 0.1447 - val_acc: 0.9394\n",
      "Epoch 6/15\n",
      "891/891 [==============================] - 42s 48ms/step - loss: 0.0201 - acc: 0.9944 - val_loss: 0.1174 - val_acc: 0.9798\n",
      "Epoch 7/15\n",
      "891/891 [==============================] - 48s 54ms/step - loss: 0.0205 - acc: 0.9955 - val_loss: 0.1198 - val_acc: 0.9495\n",
      "Epoch 8/15\n",
      "891/891 [==============================] - 48s 54ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9596\n",
      "Epoch 9/15\n",
      "891/891 [==============================] - 45s 50ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9596\n",
      "Epoch 10/15\n",
      "891/891 [==============================] - 41s 46ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9596\n",
      "Epoch 11/15\n",
      "891/891 [==============================] - 43s 49ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0816 - val_acc: 0.9495\n",
      "Epoch 12/15\n",
      "891/891 [==============================] - 40s 45ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0810 - val_acc: 0.9495\n",
      "Epoch 13/15\n",
      "891/891 [==============================] - 41s 46ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9495\n",
      "Epoch 14/15\n",
      "891/891 [==============================] - 42s 47ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0772 - val_acc: 0.9495\n",
      "Epoch 15/15\n",
      "891/891 [==============================] - 43s 48ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0775 - val_acc: 0.9596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c120ac8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# LabelEncoder 用来编码输出标签\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StratifiedShuffleSplit可以用来把数据集洗牌，并拆分成训练集和验证集\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 我们用的Keras版本是 2.0.1\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# 这个数据集是叶子leaf 品种的分类问题，有三个通道，\n",
    "# 每个通道64个比特位，一个通道代表边界特征，一个通道代表形状特征，最后一个通道代表材质特征，\n",
    "# 输出是叶子特征标签的预测\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "def encode(train, test):\n",
    "\n",
    "    # 用LabelEncoder为叶子的种类标签编码，labels对象是训练集上的标签列表\n",
    "    label_encoder = LabelEncoder().fit(train.species)\n",
    "    labels = label_encoder.transform(train.species)\n",
    "    classes = list(label_encoder.classes_)\n",
    "\n",
    "    # 此处把不必要的训练集和测试集的列删除\n",
    "    train = train.drop(['species', 'id'], axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    return train, labels, test, classes\n",
    "\n",
    "train, labels, test, classes = encode(train, test)\n",
    "\n",
    "# 这里只是标准化训练集的特征值\n",
    "scaler = StandardScaler().fit(train.values)\n",
    "scaled_train = scaler.transform(train.values)\n",
    "\n",
    "# 把数据集拆分成训练集和测试集，测试集占10%\n",
    "sss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\n",
    "for train_index, valid_index in sss.split(scaled_train, labels):\n",
    "    X_train, X_valid = scaled_train[train_index], scaled_train[valid_index]\n",
    "    y_train, y_valid = labels[train_index], labels[valid_index]\n",
    "\n",
    "# 每个输入通道的大小是64位，一共3个通道\n",
    "nb_features = 64 \n",
    "nb_class = len(classes)\n",
    "\n",
    "#  把输入数据集reshape成keras喜欢的格式：（样本数，通道大小，通道数）\n",
    "X_train_r = np.zeros((len(X_train), nb_features, 3))\n",
    "\n",
    "# 这里的做法是先把所有元素初始化成0之后，再把刚才的数据集中的数据赋值过来\n",
    "X_train_r[:, :, 0] = X_train[:, :nb_features]\n",
    "X_train_r[:, :, 1] = X_train[:, nb_features:128]\n",
    "X_train_r[:, :, 2] = X_train[:, 128:]\n",
    "\n",
    "# 验证集也要reshape一下\n",
    "X_valid_r = np.zeros((len(X_valid), nb_features, 3))\n",
    "X_valid_r[:, :, 0] = X_valid[:, :nb_features]\n",
    "X_valid_r[:, :, 1] = X_valid[:, nb_features:128]\n",
    "X_valid_r[:, :, 2] = X_valid[:, 128:]\n",
    "\n",
    "# 下面是Keras的一维卷积实现，原作者尝试过多加一些卷积层，\n",
    "# 结果并不能提高准确率，可能是因为其单个通道的信息本来就太少，深度太深的网络本来就不适合\n",
    "model = Sequential()\n",
    "\n",
    "# 一维卷积层用了512个卷积核，输入是64*3的格式\n",
    "# 此处要注意，一维卷积指的是卷积核是1维的，而不是卷积的输入是1维的，1维指的是卷积方式\n",
    "model.add(Convolution1D(nb_filter=512, filter_length=1, input_shape=(nb_features, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(nb_class))\n",
    "\n",
    "# softmax经常用来做多类分类问题\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "y_valid = np_utils.to_categorical(y_valid, nb_class)\n",
    "\n",
    "sgd = SGD(lr=0.01, nesterov=True, decay=1e-6, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "nb_epoch = 15\n",
    "model.fit(X_train_r, y_train, nb_epoch=nb_epoch, validation_data=(X_valid_r, y_valid), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
