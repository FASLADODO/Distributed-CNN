{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Demo Script\n",
    "\n",
    "This part serves as a demo for you to try the algorithm, I will keep updating this part whenever something woth powerful effect was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly, import all packages you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, load the train and test data, also, encode the label.\n",
    "\n",
    "If you do not understand the Chinese Comment, just ignore them(They are same as the English ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "def encode(train, test):\n",
    "    # Encoding the species of leaves by LabelEncoder\n",
    "    # 用LabelEncoder为叶子的种类标签编码，labels对象是训练集上的标签列表\n",
    "    label_encoder = LabelEncoder().fit(train.species)\n",
    "    labels = label_encoder.transform(train.species)\n",
    "    classes = list(label_encoder.classes_)\n",
    "    # Deleting unnecessary variables in both training and test data.\n",
    "    # 此处把不必要的训练集和测试集的列删除\n",
    "    train = train.drop(['species', 'id'], axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "    return train, labels, test, classes\n",
    "train, labels, test, classes = encode(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# 这里只是标准化训练集的特征值\n",
    "scaler = StandardScaler().fit(train.values)\n",
    "scaled_train = scaler.transform(train.values)\n",
    "# Split with splitRate=0.1\n",
    "# 把数据集拆分成训练集和测试集，测试集占10%\n",
    "sss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\n",
    "for train_index, valid_index in sss.split(scaled_train, labels):\n",
    "    X_train, X_valid = scaled_train[train_index], scaled_train[valid_index]\n",
    "    y_train, y_valid = labels[train_index], labels[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个输入通道的大小是64位，一共3个通道\n",
    "nb_features = 64 \n",
    "nb_class = len(classes)\n",
    "# Reshape ==> (Sample size,features,channel)\n",
    "#  把输入数据集reshape成keras喜欢的格式：（样本数，通道大小，通道数）\n",
    "X_train_r = np.zeros((len(X_train), nb_features, 3))\n",
    "# 这里的做法是先把所有元素初始化成0之后，再把刚才的数据集中的数据赋值过来\n",
    "X_train_r[:, :, 0] = X_train[:, :nb_features]\n",
    "X_train_r[:, :, 1] = X_train[:, nb_features:128]\n",
    "X_train_r[:, :, 2] = X_train[:, 128:]\n",
    "# reshape valid data\n",
    "# 验证集也要reshape一下\n",
    "X_valid_r = np.zeros((len(X_valid), nb_features, 3))\n",
    "X_valid_r[:, :, 0] = X_valid[:, :nb_features]\n",
    "X_valid_r[:, :, 1] = X_valid[:, nb_features:128]\n",
    "X_valid_r[:, :, 2] = X_valid[:, 128:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoder\n",
    "y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "y_valid = np_utils.to_categorical(y_valid, nb_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 64, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              67110912  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 99)                101475    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 99)                0         \n",
      "=================================================================\n",
      "Total params: 69,312,611\n",
      "Trainable params: 69,312,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# 一维卷积层用了512个卷积核，输入是64*3的格式\n",
    "# 此处要注意，一维卷积指的是卷积核是1维的，而不是卷积的输入是1维的，1维指的是卷积方式\n",
    "model.add(layers.Conv1D(input_shape=(64, 3), filters=512, kernel_size=1))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(2048, activation='relu'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(nb_class))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed settings(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform keras model into an estimator for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'dir/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x1c21cd4d68>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c21cd4e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    }
   ],
   "source": [
    "keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "      keras_model=model,\n",
    "      config=config,\n",
    "      model_dir='dir/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feed dictionary for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(X,Y,epochs, batch_size):\n",
    "    x = X\n",
    "    y = Y\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    SHUFFLE_SIZE = 5000\n",
    "    dataset = dataset.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)\n",
    "    dataset = dataset.prefetch(2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Not all devices in DistributionStrategy are visible to TensorFlow session.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='dir/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('dir/keras/keras_model.ckpt',)\n",
      "INFO:tensorflow:Warm-starting variable: conv1d/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv1d/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into dir/model.ckpt.\n",
      "INFO:tensorflow:Initialize system\n",
      "INFO:tensorflow:loss = 0.056529164, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 27 into dir/model.ckpt.\n",
      "INFO:tensorflow:Finalize system.\n",
      "INFO:tensorflow:Loss for final step: 0.056181468.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1c21df2cf8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 15\n",
    "\n",
    "keras_estimator.train(lambda:input_fn(\n",
    "                            X = X_train_r,\n",
    "                            Y = y_train,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=BATCH_SIZE)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-05-17:26:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from dir/model.ckpt-27\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-05-17:26:11\n",
      "INFO:tensorflow:Saving dict for global step 27: accuracy = 0.9898989, global_step = 27, loss = 0.05634372\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 27: dir/model.ckpt-27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9898989, 'loss': 0.05634372, 'global_step': 27}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_estimator.evaluate(lambda:input_fn(\n",
    "                                  X = X_valid_r,\n",
    "                                  Y = y_valid,\n",
    "                                  epochs=1,\n",
    "                                  batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
