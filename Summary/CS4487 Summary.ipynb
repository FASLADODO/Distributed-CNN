{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Created by **LIU Jinchao (J.C.)** at **Dec 1, 2018**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "- _Supervised learning_ considers input-output pairs $(\\mathbf{x},y)$\n",
    "  - learn a mapping from input to output.\n",
    "  - _classification_: output $y \\in \\pm 1$\n",
    "  - _regression_: output $y \\in \\mathbb{R}$\n",
    "- \"Supervised\" here means that the algorithm is learning the mapping that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "- Unsupervised learning only considers the input data $\\mathbf{x}$.\n",
    "  - There are no output values.\n",
    "- **Goal:** Try to discover inherent properties in the data.\n",
    "  - Clustering\n",
    "  - Dimensionality Reduction\n",
    "  - Manifold Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold Embedding\n",
    "- Project high-dimensional vectors into 2- or 3-dimensional space for visualization.\n",
    "  - Points in the low-dim space have similar pair-wise distances as in the high-dim space.\n",
    "- **For example:** visualize a collection of hand-written digits (images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Identifiers and Variables\n",
    "- Identifiers\n",
    "  - same as in C\n",
    "- Naming convention:\n",
    "  - `ClassName` -- a class name\n",
    "  - `varName` -- other identifier\n",
    "  - `_privateVar` -- private identifier\n",
    "  - `__veryPrivate` -- strongly private identifier\n",
    "  - `__special__` -- language-defined special name\n",
    " \n",
    "- Variables\n",
    "  - no declaration needed\n",
    "  - no need for declaring data type (automatic type)\n",
    "  - need to assign to initialize\n",
    "    - use of uninitialized variable raises exception\n",
    "  - automatic garbage collection (reference counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "a.append(4)     # add item to end\n",
    "a.pop()         # remove last item and return it\n",
    "a.insert(0,42)  # insert 42 at index 0\n",
    "del a[2]        # delete item 2\n",
    "a.reverse()     # reverse the entries\n",
    "a.sort()        # sort the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1, 2, 2, 2, 4, 5, 5]\n",
    "myList4 = [4*item for item in myList]   # multiply each item by 4\n",
    "myList4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also use conditional to select items\n",
    "[4*item*4 for item in myList if item>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1,2,'three')\n",
    "y = 4,5,6           # parentheses not needed!\n",
    "z = (1,)   # tuple with 1 element (the trailing comma is required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"he\" + \"llo\"   # concatenation\n",
    "\"hello\"*3      # repetition\n",
    "len(\"hello\")   # length\n",
    "\n",
    "\"112211\".count(\"11\")         # 2\n",
    "\"this.com\".endswith(\".com\")  # True\n",
    "\"wxyz\".startswith(\"wx\")      # True\n",
    "\"abc\".find(\"c\")              # finds first: 2\n",
    "\",\".join(['a', 'b', 'c'])    # join list: 'a,b,c'\n",
    "\"aba\".replace(\"a\", \"d\")      # replace all: \"dbd\"\n",
    "\"a,b,c\".split(',')           # make list: ['a', 'b', 'c']\n",
    "\"  abc    \".strip()          # \"abc\",  also rstrip(), lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String formatting: automatically fill in type\n",
    "\"{} and {} and {}\".format('string', 123, 1.6789)      #  'string and 123 and 1.6789'\n",
    "\n",
    "\"{:d} and {:f} and {:0.2f}\".format(False, 3, 1.234)   #  '0 and 3.000000 and 1.23'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'name': 'john', 42: 'sales', ('hello', 'world'): 6734}\n",
    "print(mydict['name'])           # get value for key 'name'\n",
    "mydict['name'] = 'jon'          # change value for key 'name'\n",
    "mydict[2] = 5                   # insert a new key-value pair\n",
    "del mydict[2]                   # delete entry for key 2\n",
    "mydict.keys()                   # iterator of all keys (no random access)\n",
    "mydict.values()                 # iterator of all values\n",
    "'name' in mydict                # check the presence of a key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Operators\n",
    "- Arithmetic: `+`, `-`, `*`, `/`, `%`, `**` (exponent), `//` (floor division)\n",
    "\n",
    "\n",
    "- Assignment: `=`, `+=`, `-=`, `/=`, `%=`, `**=`, `//=`\n",
    "- Equality: `==`, `!=`\n",
    "- Compare: `>`, `>=`, `<`, `<=`\n",
    "- Logical: `and`, `or`, `not`\n",
    "\n",
    "\n",
    "- Membership: `in`, `not in`\n",
    "\n",
    "\n",
    "- Identity: `is`, `is not`\n",
    "  - checks reference to the same object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1, 2, 2, 2, 4, 5, 5]\n",
    "sA = set(a)\n",
    "sB = {4, 5, 6, 7}\n",
    "print(sA - sB)     # set difference\n",
    "print (sA | sB)    # set union\n",
    "print (sA & sB)    # set intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['a', 'b', 'c']\n",
    "for i,n in enumerate(x):\n",
    "    print(i, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `zip` creates pairs of items between the two lists\n",
    "x = ['a', 'b', 'c']\n",
    "y = ['A', 'B', 'C']\n",
    "for i,j in zip(x,y):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'a':1, 'b':2, 'c':3}\n",
    "for (key,val) in x.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loop control (same as C)\n",
    "  - `break`, `continue`  \n",
    "- else clause\n",
    "  - runs after list is exhausted\n",
    "  - does _not_ run if loop break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? sum3        # ipython magic -- shows a help window about the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Class\n",
    "- Defining a class\n",
    "  - `self` is a reference to the object instance (passed _implicitly_)\n",
    "  \n",
    "\n",
    "- There are _no_ \"private\" members\n",
    "  - everything is accessible\n",
    "  - convention to indicate _private_:\n",
    "    - `_variable` means private method or variable (but still accessible)\n",
    "  - convention for _very private_:\n",
    "    - `__variable` is not directly visible\n",
    "    - actually it is renamed to `_classname__variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyList:\n",
    "    \"class documentation string\"\n",
    "    num = 0                  # a class variable\n",
    "    def  __init__(self, b):  # constructor\n",
    "        self.x = [b]         # an instance variable\n",
    "        MyList.num += 1      # modify class variable\n",
    "    def appendx(self, b):    # a class method\n",
    "        self.x.append(b)     # modify an instance variable\n",
    "        self.app = 1         # create new instance variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.__dict__)    # Dictionary with the namespace.\n",
    "print(c.__doc__)     # Class documentation string\n",
    "print(c.__module__)  # Module which defines the class\n",
    "print(MyList.__name__)    # Class name\n",
    "print(MyList.__bases__)   # tuple of base classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inheritence`\n",
    "\n",
    "- Multiple inheritence\n",
    "  - `class ChildClass(Parent1, Parent2, ...)`\n",
    "  - calling method in parent\n",
    "    - `super(ChildClass, self).method(args)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListAll(MyList): \n",
    "    def __init__(self, a):   # overrides MyList\n",
    "        self.allx = [a]\n",
    "        MyList.__init__(self, a)   # call base class constructor\n",
    "    def popx(self):\n",
    "        return self.x.pop()\n",
    "    def appendx(self, a):          # overrides MyList\n",
    "        self.allx.append(a)\n",
    "        MyList.appendx(self, a)    # \"super\" method call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11 Saving Objects with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file = open('blah.pickle', 'r')\n",
    "    blah = pickle.load(file)\n",
    "    file.close()\n",
    "except:               # catch everything\n",
    "    print(\"No file!\")\n",
    "else:                 # executes if no exception occurred\n",
    "    print(\"No exception!\")\n",
    "finally:\n",
    "    print(\"Bye!\")      # always executes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.12 pandas\n",
    "- pandas is a Python library for data wrangling and analysis.\n",
    "- `Dataframe` is a table of entries (like an Excel spreadsheet).\n",
    "  - each column does not need to be the same type\n",
    "  - operations to modify and operate on the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.13 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name']\n",
    "\n",
    "df[df.Age > 30]    # select Age greater than 30\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.14 NumPy\n",
    "- Library for multidimensional arrays and 2D matrices\n",
    "- `ndarray` class for multidimensional arrays\n",
    "  - elements are all the same type\n",
    "  - aliased to `array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.15 Array\n",
    "\n",
    "\n",
    "- When operating on arrays, data is sometimes copied and sometimes not.\n",
    "- _No copy is made for simple assignment._\n",
    "  - **Be careful!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = array([1,2,3,4])\n",
    "b = a               # simple assignment (no copy made!)\n",
    "b is a              # yes, b references the same object\n",
    "b[1] = -2           # changing b also changes a\n",
    "\n",
    "c = a.view()   # create a view of a\n",
    "c is a         # not the same object\n",
    "               # False\n",
    "    \n",
    "c.base is a    # but the data is owned by a\n",
    "               # True\n",
    "    \n",
    "d = a.copy()        # create a complete copy of a (new data is created)\n",
    "d is a              # not the same object\n",
    "                    # False\n",
    "d.base is a         # not sharing the same data\n",
    "                    # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.16 Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup matplotlib\n",
    "%matplotlib inline\n",
    "# setup output image format (Chrome works best)\n",
    "import IPython.core.display  \n",
    "IPython.core.display.set_matplotlib_formats(\"svg\") # file format\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = linspace(0,2*pi,16)\n",
    "y = sin(x)\n",
    "plt.plot(x, y, 'bo-')\n",
    "plt.grid(True)\n",
    "plt.ylabel('y label')\n",
    "plt.xlabel('x label')\n",
    "plt.title('my title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot string specifies three things (e.g., `'bo-'`)\n",
    "  - colors:\n",
    "    - **b**lue, **r**ed, **g**reen, **m**agenta, **c**yan, **y**ellow, blac**k**, **w**hite\n",
    "  - markers: \n",
    "    - ”.”\tpoint\n",
    "    - “o”\tcircle\n",
    "    - “v”\ttriangle down\n",
    "    - “^”\ttriangle up\n",
    "    - “<”\ttriangle left\n",
    "    - “>”\ttriangle right\n",
    "    - “8”\toctagon\n",
    "    - “s”\tsquare\n",
    "    - “p”\tpentagon\n",
    "    - “*”\tstar\n",
    "    - “h”\thexagon1\n",
    "    - “+”\tplus\n",
    "    - “x”\tx\n",
    "    - “d”\tthin_diamond\n",
    "  - line styles:\n",
    "    - '-' solid line\n",
    "    - '--' dashed line\n",
    "    - '-.' dash-dotted line\n",
    "    - ':' dotted lione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.17 Split\n",
    "\n",
    "  - use `model_selection` module\n",
    "    - `train_test_split` - give the percentage for training and testing.\n",
    "    - `StratifiedShuffleSplit` - also preserves the percentage of examples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = \\\n",
    "  model_selection.train_test_split(X, Y, \n",
    "     train_size=0.5, test_size=0.5, random_state=4487)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.18 Text Document Representation (Bag of Words)\n",
    "\n",
    "- `Bag-of-Words (BoW) model`\n",
    "  - Let ${\\cal V}=\\{w_1, w_2, \\cdots w_V\\}$ be a list of $V$ words (called a **vocabulary**).\n",
    "  - represent a text document as a vector $\\mathbf{x}\\in{\\mathbb R}^V$.\n",
    "    - each entry $x_j$ represents the number of times word $w_j$ appears in the document.\n",
    "- Example:\n",
    "  - Document: \"This is a test document\"\n",
    "  - Vocabulary: ${\\cal V} = \\{\\textrm{\"this\"}, \\textrm{\"test\"}, \\textrm{\"spam\"}, \\textrm{\"foo\"}\\}$\n",
    "  - Vector representation: $\\mathbf{x}=[1, 1, 0, 0]$\n",
    "  \n",
    "  \n",
    "- NOTE:\n",
    "  - `the order of the words is not used!`\n",
    "  - rearranging words leads to the same representation!\n",
    "- Example: \n",
    "  - \"this is spam\"  $\\rightarrow \\mathbf{x}=[1, 0, 1, 0]$\n",
    "  - \"is this spam\" $\\rightarrow \\mathbf{x}=[1, 0, 1, 0]$\n",
    "- This is why it is called \"bag-of-words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the document vectorizer to make BoW\n",
    "# - use english stop words\n",
    "# - only use the most frequent 100 words in the dataset\n",
    "cntvect = feature_extraction.text.CountVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "# create the vocabulary, and return the document vectors\n",
    "# NOTE: we only use the training data!\n",
    "trainX = cntvect.fit_transform(traintext)\n",
    "\n",
    "# calculate vectors for the test data\n",
    "testX  = cntvect.transform(testtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.19 Term-Frequency Inverse Document Frequency (TF-IDF)\n",
    "  - some words are common among many documents\n",
    "    - common words are less informative because they appear in both classes.\n",
    "  - `inverse document frequency (IDF)` - measure rarity of each word\n",
    "    - $IDF(j) = \\log \\frac{N}{N_j}$\n",
    "      - $N$ is the number of documents.\n",
    "      - $N_j$ is the number of documents with word $j$.\n",
    "    - IDF is:\n",
    "      - 0 when a word is common to all documents\n",
    "      - large value when the word appears in few documents\n",
    "  - `TF-IDF vector`: downscale words that are common in many documents\n",
    "    - multiply TF and IDF terms\n",
    "    - $x_j = \\frac{w_j}{|D|} \\log \\frac{N}{N_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF representation\n",
    "tf_trans = feature_extraction.text.TfidfTransformer(norm='l1', use_idf=False)\n",
    "Xtf = tf_trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "# (For TF, pass use_idf=False)\n",
    "tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
    "# 'l1' - entries sum to 1\n",
    "\n",
    "# setup the TF-IDF representation, and transform the training set\n",
    "trainXtf = tf_trans.fit_transform(trainX)\n",
    "\n",
    "# transform the test set\n",
    "testXtf = tf_trans.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Other text preprocessing`\n",
    "  - _Stemming_\n",
    "    - convert related words into a common root word\n",
    "    - example: testing, tests --> \"test\"\n",
    "    - see NLTK toolbox (http://www.nltk.org)\n",
    "  - _Lemmatisation_\n",
    "    - similar to stemming\n",
    "    - groups inflections of word together (gone, going, went -> go)\n",
    "    - see NLTK\n",
    "  - Removing numbers and punctuation.\n",
    "\n",
    "- ** Other word models **\n",
    "  - _N-grams_\n",
    "    - similar to BoW except look at pairs of consecutive words (or N consecutive words in general)\n",
    "  - _word vectors_ \n",
    "    - each word is a real vector, where direction indicates the \"concept\"\n",
    "    - words about similar things point in the same direction\n",
    "    - adding and subtracting word vectors yield new word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.20 Cross-validation\n",
    "- Use _cross-validation_ on the training set to select the best value of $C$.\n",
    "- Run many experiments on the training set to see which parameters work on different versions of the data.\n",
    "  - Split the data into batches of training and validation data.\n",
    "  - Try a range of $C$ values on each split.\n",
    "  - Pick the value that works best over all splits.\n",
    "\n",
    "<center><img src=\"imgs/10_fold_cv.png\" width=\"500px\"/></center>\n",
    "\n",
    "\n",
    "- **Procedure**\n",
    "  1. select a range of $C$ values to try\n",
    "  2. Repeat $K$ times\n",
    "    1. Split the training set into training data and validation data\n",
    "    2. Learn a classifier for each value of $C$\n",
    "    3. Record the accuracy on the validation data for each $C$\n",
    "  3. Select the value of $C$ that has the highest average accuracy over all $K$ folds.\n",
    "  4. Retrain the classifier using all data and the selected $C$.\n",
    "\n",
    "\n",
    "- scikit-learn already has built-in `cross_validation` module (more later).\n",
    "- for logistic regression, use _LogisticRegressionCV_ class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.21 Feature Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Method 1:** scale each feature dimension so the mean is 0 and variance is 1.\n",
    "    - $\\tilde{x}_d = \\frac{1}{s}(x_d-m)$\n",
    "    - $s$ is the standard deviation of feature values.\n",
    "    - $m$ is the mean of the feature values.\n",
    "- **NOTE:** the parameters for scaling the features should be estimated from the training set!\n",
    "  - same scaling is applied to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the iris data\n",
    "scaler = preprocessing.StandardScaler()  # make scaling object\n",
    "trainXn = scaler.fit_transform(trainX)   # use training data to fit scaling parameters\n",
    "testXn  = scaler.transform(testX)        # apply scaling to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Method 2:** scale features to a fixed range, -1 to 1.\n",
    "  - $\\tilde{x}_d = 2*(x_d - min) / (max-min) - 1$\n",
    "  - $max$ and $min$ are the maximum and minimum features values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the iris data\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))    # make scaling object\n",
    "trainXn = scaler.fit_transform(trainX)   # use training data to fit scaling parameters\n",
    "testXn  = scaler.transform(testX)        # apply scaling to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.22 One-hot encoding\n",
    "- encode a categorical variable as a vector of ones and zeros\n",
    "  - if there are $K$ categories, then the vector is $K$ dimensions.\n",
    "- Example:\n",
    "  - x=cat $\\rightarrow$ x=[1 0 0]\n",
    "  - x=dog $\\rightarrow$ x=[0 1 0]\n",
    "  - x=horse $\\rightarrow$ x=[0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding example\n",
    "X = [[0], [1], [0], [2], [2]]  # original categorical data {0,1,2}\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "ohe.fit(X)         # finds the number of categories in the training set: 0-max(X)\n",
    "ohe.transform(X)   # transform to one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.23 Binning\n",
    "- encode a real value as a vector of ones and zeros\n",
    "  - assign each feature value to a bin, and then use one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "X = [[-3], [0.5], [1.5]]  # the data\n",
    "bins = [-2,-1,0,1,2]      # define the bins\n",
    "\n",
    "# map from value to bin number\n",
    "Xbins = digitize(X, bins=bins)   \n",
    "\n",
    "# map from bin number to 0-1 vector\n",
    "ohe = preprocessing.OneHotEncoder(n_values=len(bins), sparse=False)\n",
    "ohe.fit(Xbins)\n",
    "ohe.transform(Xbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.24 Data transformations - polynomials\n",
    "- Represent interactions between features using polynomials\n",
    "- Example:\n",
    "  - 2nd-degree polynomial models pair-wise interactions\n",
    "    - $[x_1, x_2] \\rightarrow [x_1^2, x_1 x_2, x_2^2]$\n",
    "  - Combine with other degrees:\n",
    "    - $[x_1, x_2] \\rightarrow [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0,1], [1,2], [3,4]]\n",
    "pf = preprocessing.PolynomialFeatures(degree=2)\n",
    "pf.fit(X)\n",
    "pf.transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Classification Problem\n",
    "- Observation $\\mathbf{x}$ (i.e., features)\n",
    "  - typically a real vector, $\\mathbf{x}\\in \\mathbb{R}^d$.\n",
    "  - **Example**: a 2-dim vector containing the petal length and sepal width.\n",
    "    - $\\mathbf{x} = \\begin{bmatrix}\\text{petal length}\\\\ \\text{sepal width}\\end{bmatrix} = \\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}$\n",
    "- Class $y$\n",
    "  - takes values from a set of possible class labels $\\cal{Y}$.\n",
    "  - **Example:** $\\cal{Y}=\\{\\textrm{\"versicolor\"}, \\textrm{\"virginica\"}\\}$.\n",
    "    - or equivalently as numbers, $\\cal{Y}=\\{1,2\\}$.\n",
    "- **Goal**: given an observed features $\\mathbf{x}$, predict its class $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Linear Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic model\n",
    "- Model _how_ the data is generated using probability distributions.\n",
    "  - called a **generative model**.\n",
    "- Generative model\n",
    "  - 1) The world has objects of various classes.\n",
    "  - 2) The observer measures features/observations from the objects.\n",
    "  - 3) Each class of objects has a particular distribution of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "- in the world, the frequency that class $y$ occurs is given by the probability distribution $p(y)$.\n",
    "  - $p(y)$ is called the **prior distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the observation is drawn according to the distribution $p(\\mathbf{x}|y)$.\n",
    "  - $p(x|y)$ is called the **class conditional distribution**\n",
    "    - \"probability of observing a particular feature vector $\\mathbf{x}$ given the object is class $y$\"\n",
    "    - can \"smooth out the samples\" or \"fill-in\" values between samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian distribution (normal distribution)\n",
    "- Each class is modeled as a separate Gaussian distribution of the feature value\n",
    "  - $p(x|y=c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}}e^{-\\frac{1}{2\\sigma_c^2}(x-\\mu_c)^2}$\n",
    "  - Each class has its own mean and variance parameters $(\\mu_c, \\sigma_c^2)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimation (MLE)\n",
    "  - set the parameters ($\\mu$, $\\sigma^2$) to maximize the likelihood (probability) of the samples for that class.\n",
    "  - Let $\\{\\mathbf{x}_i, y_i\\}_{i=1}^N$ be the data for one class:\n",
    "  $$ (\\hat{\\mu},\\hat{\\sigma}^2) = \\mathop{\\mathrm{argmax}}_{\\mu, \\sigma^2} \\sum_{i=1}^N \\log p(\\mathbf{x}_i| y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Solution: \n",
    "  - sample mean: $\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i$\n",
    "  - sample variance: $\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{x}_i - \\hat{\\mu})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Decision Rule\n",
    "- The Bayesian decision rule (BDR) makes the optimal decisions on problems involving probability (uncertainty).\n",
    "  - minimizes the _probability of making a prediction error_.\n",
    "- **Bayes Classifier**\n",
    "  - Given observation $x$, pick the class $c$ with the _largest posterior probability_, $p(y=c|x)$.\n",
    "  - **Example:**\n",
    "    - if $p(y=1|x) > p(y=2|x)$, then choose Class 1\n",
    "    - if $p(y=1|x) < p(y=2|x)$, then choose Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes' Rule\n",
    "- The posterior probability can be calculated using Bayes' rule:\n",
    "$$p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "  - The denominator is the probability of $x$:\n",
    "    - $p(x) = \\sum_{y\\in {\\cal Y}} p(x|y)p(y)$\n",
    "  - The denominator makes $p(y|x)$ sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayes' rule: $$p(y|x) = \\frac{p(x|y)p(y)}{p(x|y=1)p(y=1) + p(x|y=2)p(y=2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example:**\n",
    "  - BDR using joint likelihoods:\n",
    "    - if $p(x|y=1)p(y=1)$ $>$ $p(x|y=2)p(y=2)$, then choose Class 1\n",
    "    - otherwise, choose Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can also apply a monotonic increasing function (like $\\log$) and do the comparison.\n",
    "  - Using log likelihoods:\n",
    "    - $\\log p(x|y=1)+\\log p(y=1)$ $>$ $\\log p(x|y=2)+\\log p(y=2)$\n",
    "  - This is more numerically stable when the likelihoods are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training:**\n",
    "  1. Collect training data from each class. \n",
    "  2. For each class $c$, estimate the class conditional densities $p(x|y=c)$:\n",
    "    1. select a form of the distribution (e.g. Gaussian).\n",
    "    2. estimate its parameters with MLE.\n",
    "  3. Estimate the class priors $p(y)$ using MLE.\n",
    "- **Classification:**\n",
    "  1. Given a new sample $x^*$, calculate the likelihood $p(x^*|y=c)$ for each class $c$.\n",
    "  2. Pick the class $c$ with largest posterior probability $p(y=c|x)$.\n",
    "    - (equivalently, use $p(x|y=c)p(y=c)$ or $\\log p(x|y=c)+\\log p(y=c)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.1 Gaussian NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the NB Gaussian model from sklearn\n",
    "model = naive_bayes.GaussianNB()\n",
    "\n",
    "# fit the model to training data\n",
    "model.fit(trainX, trainY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the model\n",
    "predY = model.predict(testX)\n",
    "# calculate accuracy\n",
    "acc = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes model for Boolean vectors\n",
    "- Model each word independently\n",
    "  - absence/presence of a word $w_j$ in document\n",
    "  - Bernoulli distribution\n",
    "    - present: $p(x_j=1|y) = \\pi_j$\n",
    "    - absent: $p(x_j=0|y) = 1-\\pi_j$\n",
    "  - MLE parameters: $\\pi_j=N_j/N$, \n",
    "    - $N_j$ is the number of documents in class $y$ that contain word $j$.\n",
    "    - $N$ is the number of documents in class $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class-conditional distribution\n",
    "   $$p(x_1,\\cdots,x_V|y=\\textrm{spam}) = \\prod_{j=1}^V p(x_j|y=\\textrm{spam})$$\n",
    "   $$\\log p(x_1,\\cdots,x_V|y=\\textrm{spam}) = \\sum_{j=1}^V \\log p(x_j|y=\\textrm{spam})$$\n",
    "- for a document, the log-probabilities of the words being in a spam message adds.\n",
    "  - accumulate evidence over all words in the document.\n",
    "  - more words that are associated with spam --> more likely the document is spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.2 Bernoulli NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the NB Bernoulli model.\n",
    "# the model automatically converts count vector into binary vector\n",
    "bmodel = naive_bayes   .BernoulliNB(alpha=0.0)\n",
    "bmodel.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "predY = bmodel.predict(testX)\n",
    "# calculate accuracy\n",
    "acc = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Smoothing`\n",
    "- Some words are not present in any documents for a given class.\n",
    "  - $N_j=0$, and thus $\\pi_j = 0$.\n",
    "    - i.e., the document in the class **definitely** will not contain the word.\n",
    "    - can be a problem since we simply may not have seem an example with that word.\n",
    "- Smoothed MLE\n",
    "  - add a smoothing parameter $\\alpha$ that adds a \"virtual\" count\n",
    "  - parameter: $\\pi_j= (N_j+\\alpha)/(N+2\\alpha)$, \n",
    "  - this is called _Laplace smoothing_\n",
    "- In general, _regularizing_ or _smoothing_ of the estimate helps to prevent _overfitting_ of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- **Generative classification model**\n",
    "  - estimate probability distributions of features generated from each class.\n",
    "  - given feature observation predict class with largest posterior probability.\n",
    "- **Advantages:**\n",
    "  - works with small amount of data.\n",
    "  - works with multiple classes.\n",
    "- **Disadvantages:**\n",
    "  - accuracy depends on selecting an appropriate probability distribution.\n",
    "    - if the probability distribution doesn't model the data well, then accuracy might be bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Terminology\n",
    "  - **\"Discriminative\"** - learn to directly discriminate the classes apart using the features.\n",
    "  - **\"Generative\"** - learn model of how the features are generated from different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Logistic regression\n",
    "- Use a probabilistic approach\n",
    "- Need to map the function values $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + b$ to probability values between 0 and 1.\n",
    "  - _sigmoid_ function maps from real number to interval [0,1]\n",
    "  - $\\sigma(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a feature vector $x$, the probability of a class is:\n",
    "  - $p(y=+1|\\mathbf{x}) = \\sigma( f(\\mathbf{x}) )$\n",
    "  - $p(y=-1|\\mathbf{x}) = 1-\\sigma( f(\\mathbf{x}) )$\n",
    "- Note: here we are directly modeling the class posterior probability!\n",
    "  - not the class-conditional $p(\\mathbf{x}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Learning the parameters`\n",
    "- Given training data $\\{\\mathbf{x}_i,y_i\\}_{i=1}^N$, learn the function parameters $(\\mathbf{w},b)$ using maximum likelihood estimation.\n",
    "- maximize the likelihood of the data $\\{\\mathbf{x}_i,y_i\\}$:\n",
    "  $$(\\mathbf{w}^*,b^*) = \\mathop{\\mathrm{argmax}}_{\\mathbf{w},b} \\sum_{i=1}^N \\log p(y_i|\\mathbf{x}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to prevent _overfitting_, add a prior distribution on $\\mathbf{w}$.\n",
    "  - assume Gaussian distribution on $\\mathbf{w}$ with variance $1/C$\n",
    "  $$(\\mathbf{w}^*,b^*) = \\mathop{\\mathrm{argmax}}_{\\mathbf{w},b} \\log p(\\mathbf{w}) + \\sum_{i=1}^N \\log p(y_i|\\mathbf{x}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equivalently, \n",
    "  $$(\\mathbf{w}^*,b^*) = \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{C} \\mathbf{w}^T\\mathbf{w} + \\sum_{i=1}^N \\log (1+\\exp(-y_i (\\mathbf{w}^T \\mathbf{x}_i+b)))$$\n",
    "- the first term is the _regularization term_\n",
    "  - Note: $\\mathbf{w}^T\\mathbf{w} = \\sum_{j=1}^d w_j^2$\n",
    "  - penalty term that keeps entries in $\\mathbf{w}$ from getting too large.\n",
    "  - $C$ is the regularization _hyperparameter_\n",
    "    - larger $C$ value allow large values in $\\mathbf{w}$.\n",
    "    - smaller $C$ value discourage large values in $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $$(\\mathbf{w}^*,b^*) = \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{C} \\mathbf{w}^T\\mathbf{w} + \\sum_{i=1}^N \\log (1+\\exp(-y_i (\\mathbf{w}^T \\mathbf{x}_i+b)))$$\n",
    "\n",
    "\n",
    "- the second term is the _data-fit term_\n",
    "    - wants to make the parameters $(\\mathbf{w},b)$ to well fit the data.\n",
    "    - Define $z_i = y_i f(\\mathbf{x}_i)$\n",
    "      - Interesing observation:\n",
    "        - $z_i > 0$ when sample $\\mathbf{x}_i$ is classified correctly\n",
    "        - $z_i < 0$ when sample $\\mathbf{x}_i$ is classified incorrectly\n",
    "        - $z_i = 0$ when sample is on classifier boundary\n",
    "    - logistic loss function: $L(z_i) = \\log (1+\\exp(-z_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **no closed-form solution**\n",
    "  - use an iterative optimization algorithm to find the optimal solution\n",
    "  - e.g. _gradient descent_ - step downhill in each iteration.\n",
    "    - $\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{d E}{d\\mathbf{w}}$\n",
    "    - where $E$ is the objective function\n",
    "    - $\\eta$ is the _learning rate_ (how far to step in each iteration).\n",
    "    \n",
    "    \n",
    "      \n",
    "<left><img src=\"imgs/LG-1.png\" width=\"500px\"/></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn logistic regression classifier\n",
    "# (C is a regularization hyperparameter)\n",
    "logreg = linear_model.LogisticRegression(C=100)\n",
    "logreg.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the model\n",
    "predY = logreg.predict(testX)\n",
    "\n",
    "# calculate accuracy\n",
    "acc = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross-validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn logistic regression classifier usinc CV\n",
    "#  Cs is an array of possible C values\n",
    "#  cv is the number of folds\n",
    "#  n_jobs is the number of parallel jobs to run (makes it faster)\n",
    "#    -1 means use all cores\n",
    "logreg = linear_model.LogisticRegressionCV(Cs=logspace(-4,4,20), cv=5, n_jobs=-1)\n",
    "logreg.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class classification\n",
    "\n",
    "- For more than 2 classes, split the problem up into several binary classifier problems.\n",
    "  - **1-vs-rest** \n",
    "    - _Training:_ for each class, train a classifier for that class versus the other classes.  \n",
    "      - For example, if there are 3 classes, then train 3 binary classifiers:  1 vs {2,3}; 2 vs {1,3}; 3 vs {1,2}\n",
    "    - _Prediction:_ calculate probability for each binary classifier.  Select the class with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn logistic regression classifier (one-vs-all)\n",
    "mlogreg = linear_model.LogisticRegression(C=10) # Same\n",
    "mlogreg.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass logistic regression\n",
    "- Another way to get a multi-class classifier is to define a multi-class objective.\n",
    "  - One weight vector $\\mathbf{w}_c$ for each class c.\n",
    "- Define probabilities with **softmax** function\n",
    "  - analogous to sigmoid function for binary logistic regression.\n",
    "  - $p(y=c|\\mathbf{x}) = \\frac{\\exp (\\mathbf{w}_c^T \\mathbf{x})}{\\exp (\\mathbf{w}_1^T \\mathbf{x}) + \\cdots + \\exp (\\mathbf{w}_K^T \\mathbf{x})}$\n",
    "  - The class with largest reponse of $\\mathbf{w}_c^T\\mathbf{x}$ will have the highest probability.\n",
    "- Estimate the $\\{\\mathbf{w}_j\\}$ parameters using MLE as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn logistic regression classifier\n",
    "mlogreg = linear_model.LogisticRegression(C=10, \n",
    "            multi_class='multinomial', solver='lbfgs')  \n",
    "            # use multi-class and corresponding solver\n",
    "mlogreg.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum margin\n",
    "- Define the space between the separating line and the closest point as the _margin_.\n",
    "  - think of this space as the \"amount of wiggle room\" for accomodating errors in estimating $\\mathbf{w}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Idea:** the best separating line is the one that _maximizes the margin_.\n",
    "  - i.e., puts the most distance between the closest points and the decision boundary.\n",
    "  \n",
    "  \n",
    "  \n",
    "<left><img src=\"imgs/SVM-1.png\" width=\"500px\"/></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Training\n",
    "- given a training set $\\{\\mathbf{x}_i,y_i\\}_{i=1}^N$, optimize:\n",
    "  $$ \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{2} \\mathbf{w}^T\\mathbf{w}\\\\\\mathrm{s.t.}~y_i(\\mathbf{w}^T\\mathbf{x}_i+b)\\geq 1,\\quad 1\\leq i \\leq N$$\n",
    "  - the objective minimizes the inverse of the margin distance, i.e., maximizes the margin.\n",
    "  - the inequality constraints ensure that all points are either on or outside of the margin.\n",
    "    - the margin is set to be distance of 1 from the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Prediction\n",
    "- given a new data point $\\mathbf{x}_*$, use sign of linear function to predict class\n",
    "    - $y_* = \\mathop{\\mathrm{sign}}(\\mathbf{w}^T \\mathbf{x}_* + b)$\n",
    "#### Why is maximizing the margin good?\n",
    "- the true $\\mathbf{w}$ is uncertain\n",
    "  - maximizing the margin allows the most uncertainty (wiggle room) for $\\mathbf{w}$, while keeping all the points correctly classified.\n",
    "  \n",
    "- the data points are uncertain\n",
    "  - maximizing the margin allows the most wiggle of the points, while keeping all the points correctly classified.\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "<left><img src=\"imgs/SVM-2.png\" width=\"500px\"/></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about non-separable data?\n",
    "- use the same linear classifier\n",
    "  - allow some training samples to violate margin\n",
    "    - i.e., are inside the margin (or even mis-classified)\n",
    "  - Define \"slack\" variable $\\xi_i \\geq 0$\n",
    "    - $\\xi_i=0$ means sample is outside of margin area (no slack)\n",
    "    - $\\xi_i>0$ means sample is inside of margin area (slack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - introduce a parameter $C$ which is the penalty for each training sample that violates the margin.\n",
    "    - smaller value means allow more violations (less penalty)\n",
    "    - larger value means don't allow violations (more penalty)\n",
    "\n",
    "$$ \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{2} \\mathbf{w}^T\\mathbf{w} + C\\sum_{i=1}^N \\xi_i \\\\\\mathrm{s.t.}~y_i(\\mathbf{w}^T\\mathbf{x}_i+b)\\geq 1-\\xi_i,\\quad 1\\leq i \\leq N\\\\\\xi_i\\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "- After some massaging, the objective function is:\n",
    "$$ \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{C} \\mathbf{w}^T\\mathbf{w} + \\sum_{i=1}^N \\max(0, 1-y_i (\\mathbf{w}^T\\mathbf{x}_i + b))$$\n",
    "  - hinge loss function: $L(z_i) = \\max(0,1-z_i)$\n",
    "    - Note: $\\max(a,b)$ returns whichever value ($a$ or $b$) is largest.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<left><img src=\"imgs/SVM-3.png\" width=\"500px\"/></left>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM doesn't have it's own dedicated cross-validation function\n",
    "- Use the `GridSearchCV` to run cross-validation for a list of parameters\n",
    "  - calculate average accuracy for each parameter\n",
    "  - select parameter with highest accuracy, retrain model with all data\n",
    "  - Speed up: each parameter can be trained/tested separately, specify number of parallel jobs using `n_jobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the list of parameters to try\n",
    "paramgrid = {'C': logspace(-3,3,13)}\n",
    "\n",
    "# setup the cross-validation object\n",
    "# pass the SVM object, parameter grid, and number of CV folds\n",
    "# set number of parallel jobs to -1 (use all cores)\n",
    "svmcv = model_selection.GridSearchCV(svm.SVC(kernel='linear'), paramgrid, cv=5,\n",
    "                                     n_jobs=-1, verbose=True)\n",
    "\n",
    "# run cross-validation (train for each split)\n",
    "svmcv.fit(trainX, trainY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view best results and best retrained estimator\n",
    "svmcv.best_params_\n",
    "svmcv.best_score_\n",
    "svmcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly use svmcv to make predictions\n",
    "predY = svmcv.predict(testX)\n",
    "\n",
    "acc = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class SVM\n",
    "- In sklearn, `svm.SVC` implements  \"1-vs-1\" multi-class classification.\n",
    "  - Train binary classifiers on all pairs of classes.\n",
    "    - 3-class Example: 1 vs 2, 1 vs 3, 2 vs 3\n",
    "  - To label a sample, pick the class with the most votes among the binary classifiers.\n",
    "- Problem:\n",
    "  - 1v1 classification is very slow when there are a large number of classes.\n",
    "    - if there are $C$ classes, need to train $C(C-1)/2$ binary classifiers!\n",
    "    \n",
    "    \n",
    "    \n",
    "#### 1-vs-all SVM\n",
    "- Use the `multiclass.OneVsRestClassifier` to build a 1-vs-all classifier from any binary classifier.\n",
    "  - For `GridSearchCV`, use `'estimator__C'` as the parameter label for `C` in the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msvm = multiclass.OneVsRestClassifier(svm.SVC(kernel='linear'))\n",
    "\n",
    "# setup the parameters and run CV\n",
    "paramgrid = {'estimator__C': logspace(-3,3,13)}\n",
    "msvmcv = model_selection.GridSearchCV(msvm, paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
    "msvmcv.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary for LR & SVM\n",
    "\n",
    "\n",
    "\n",
    "- **Linear classifiers:**\n",
    "  - separate the data using a linear surface (hyperplane).\n",
    "  - $y = \\mathrm{sign}(\\mathbf{w}^T\\mathbf{x} + b)$\n",
    "- **Two formulations:**\n",
    "  - logistic regression - maximize the probability of the data\n",
    "  - support vector machine - maximize the margin of the hyperplane\n",
    "  \n",
    "- **Loss functions**\n",
    "  - SVM - ensure a margin of 1 between boundary and closest point\n",
    "  - LR - push the classification boundary as far as possible from all points\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  <left><img src=\"imgs/SVM-4.png\" width=\"500px\"/></left>  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "- **Advantages:**\n",
    "  - SVM works well on high-dimensional features ($d$ large), and has low generalization error.\n",
    "  - LR has well-calibrated probabilities.\n",
    "- **Disadvantages:**\n",
    "  - decision surface can only be linear! \n",
    "    - Next lecture we will see how to deal with non-linear decision surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Non-Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea - transform the input space\n",
    "- map from input space $\\mathbf{x} \\in \\mathbb{R}^d$ to a new high-dimensional space $\\mathbf{z} \\in \\mathbb{R}^D$.\n",
    "  - $\\mathbf{z} = \\Phi(\\mathbf{x})$, where $\\Phi(\\mathbf{x})$ is the transformation function.\n",
    "- learn the linear classifier in the new space\n",
    "  - if dimension of new space is large enough ($D>d$), then the data should be linearly separable\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "<left><img src=\"imgs/NLC-1.png\" width=\"700px\"/></left> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with transformed input\n",
    "- Given a training set $\\{\\mathbf{x}_i,y_i\\}_{i=1}^N$, the original SVM training is:\n",
    "  $$ \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{2} \\mathbf{w}^T\\mathbf{w}\\quad \\mathrm{s.t.}~y_i(\\mathbf{w}^T\\mathbf{x}_i+b)\\geq 1,\\quad 1\\leq i \\leq N$$\n",
    "- Apply high-dimensional transform to input $\\mathbf{x}\\rightarrow \\Phi(\\mathbf{x})$:\n",
    "  $$ \\mathop{\\mathrm{argmin}}_{\\mathbf{w},b} \\frac{1}{2} \\mathbf{w}^T\\mathbf{w}\\quad \\mathrm{s.t.}~y_i(\\mathbf{w}^T\\Phi(\\mathbf{x}_i)+b)\\geq 1,\\quad 1\\leq i \\leq N$$\n",
    "  \n",
    "  \n",
    "#### SVM Dual Problem\n",
    "- Using some convex optimization theory, the SVM problem can be rewritten as a _dual_ problem:\n",
    "$$\\mathop{\\mathrm{argmax}}_{\\alpha} \\sum_i \\alpha_i -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_j y_i y_j \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j) \\\\ \\mathrm{s.t.} \\sum_{i=1}^N \\alpha_iy_i = 0, \\quad \\alpha_i\\geq 0$$\n",
    "- The new variable $\\alpha_i$ corresponds to each training sample $(\\mathbf{x}_i,y_i)$.\n",
    "\n",
    "\n",
    "\n",
    "- Recover the hyperplane $w$ as a sum of the margin points:\n",
    "  - $\\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\Phi(\\mathbf{x}_i)$\n",
    "- Classify a new point $\\mathbf{x}_*$,  \n",
    "  - $\\begin{align}y_* &= \\mathrm{sign}(\\mathbf{w}^T\\Phi(\\mathbf{x}_*)+b) \\\\&= \\mathrm{sign}(\\sum_{i=1}^N \\alpha_i y_i \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_*) + b)\\end{align}$\n",
    "- Interpretation of $\\alpha_i$\n",
    "  - $\\alpha_i=0$ when the sample $\\mathbf{x}_i$ is not on the margin.\n",
    "  - $\\alpha_i>0$ when the sample $\\mathbf{x}_i$ is on the margin (or violates).\n",
    "    - i.e., the sample $\\mathbf{x}_i$ is a support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel function\n",
    "- the SVM dual problem is completely written in terms of _inner product_ between the high-dimensional feature vectors: $\\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j)$\n",
    "- So rather than explicitly calculate the high-dimensional vector $\\Phi(\\mathbf{x}_i)$, \n",
    "  - we only need to calculate the inner product between two high-dim feature vectors.\n",
    "- We call this a **kernel function**\n",
    "  - $k(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j)$\n",
    "  - calculating the kernel will be less expensive than explicitly calculating the high-dimensional feature vector and the inner product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Polynomial kernel\n",
    "- input vector $\\mathbf{x}=\\left[\\begin{matrix}x_1\\\\\\vdots\\\\x_d\\end{matrix}\\right]\\in\\mathbb{R}^d$\n",
    "- kernel between two vectors is a $p$-th order polynomial:\n",
    "  - $k(\\mathbf{x},\\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x}')^p = (\\sum_{i=1}^d x_i x_i')^p$\n",
    "                                                                \n",
    "                             \n",
    "- For example, $p=2$,\n",
    "  $$\\begin{align}k(\\mathbf{x},\\mathbf{x}') &= (\\mathbf{x}^T \\mathbf{x}')^2 = (\\sum_{i=1}^d x_i x_i')^2 \\\\ &= \\sum_{i=1}^d \\sum_{j=1}^d (x_ix_i' x_jx_j') = \\Phi(\\mathbf{x})^T \\Phi(\\mathbf{x}')\\end{align}$$\n",
    "  - transformed feature space is the quadratic terms of the input vector: \n",
    "    $$ \\Phi(\\mathbf{x}) = \\left[\\begin{matrix} x_1 x_1 \\\\ x_1 x_2 \\\\ \\vdots  \\\\\n",
    "    x_2 x_1 \\\\x_2 x_2 \\\\ \\vdots \\\\\n",
    "    x_d x_1 \\\\ \\vdots \\\\ x_d x_d\n",
    "    \\end{matrix}\\right]\n",
    "    $$\n",
    "- Comparison of number of multiplications\n",
    "  - for kernel: $O(d)$\n",
    "  - explicit transformation $\\Phi$: $O(d^2)$\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel trick\n",
    "- Replacing the inner product with a kernel function in the optimization problem is called the **kernel trick**.\n",
    "  - _turns a linear classification algorithm into a non-linear classification algorithm_.\n",
    "  - the shape of the decision boundary is determined by the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Kernel SVM\n",
    "- Replace inner product in linear SVM with kernel function:\n",
    "  $$\\mathop{\\mathrm{argmax}}_{\\alpha} \\sum_i \\alpha_i -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_j y_i y_j k(\\mathbf{x}_i, \\mathbf{x}_j) \\\\ \\mathrm{s.t.} \\sum_{i=1}^N \\alpha_iy_i = 0, \\quad \\alpha_i\\geq 0$$\n",
    "- Prediction\n",
    "  - $y_* = \\mathrm{sign}(\\sum_{i=1}^N \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x}_*) + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF kernel\n",
    "- RBF kernel (radial basis function)\n",
    "  - $k(\\mathbf{x},\\mathbf{x}') = e^{-\\gamma\\|\\mathbf{x}-\\mathbf{x}'\\|^2}$\n",
    "  - similar to a Gaussian\n",
    "- gamma $\\gamma>0$ is the inverse bandwidth parameter of the kernel\n",
    "  - controls the smoothness of the function\n",
    "  - small $\\gamma$ &#8594; wider Gaussian &#8594; smooth functions\n",
    "  - large $\\gamma$ &#8594; thin Gaussian &#8594; wiggly function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the list of parameters to try\n",
    "paramgrid = {'C': logspace(-2,3,20), \n",
    "             'gamma': logspace(-4,3,20) }\n",
    "\n",
    "# setup the cross-validation object\n",
    "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
    "svmcv = model_selection.GridSearchCV(svm.SVC(kernel='rbf'), paramgrid, cv=5,\n",
    "                                    n_jobs=-1, verbose=True)\n",
    "\n",
    "# run cross-validation (train for each split)\n",
    "svmcv.fit(trainX, trainY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the list of parameters to try\n",
    "paramgrid = {'C': logspace(-2,3,20), \n",
    "             'degree': [2,3,4] }\n",
    "\n",
    "# setup the cross-validation object\n",
    "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
    "svmcv = model_selection.GridSearchCV(svm.SVC(kernel='poly'), paramgrid, cv=5,\n",
    "                                    n_jobs=-1, verbose=True)\n",
    "\n",
    "# run cross-validation (train for each split)\n",
    "svmcv.fit(trainX, trainY);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Advantages:**\n",
    "  - non-linear decision boundary for more complex classification problems\n",
    "  - some intuition from the type of kernel function used.\n",
    "  - kernel function can also be used to do non-vector data.\n",
    "- **Disadvantages:**\n",
    "  - sensitive to the kernel function used.\n",
    "  - sensitive to the $C$ and kernel hyperparameters.\n",
    "  - computationally expensive to do cross-validation.\n",
    "  - need to calculate the kernel matrix\n",
    "    - $N^2$ terms where $N$ is the size of the training set\n",
    "    - for large $N$, uses a large amount of memory and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Kernels on other types of data\n",
    "- **Histograms:** $\\mathbf{x} = [x_1,\\cdots, x_d]$, $x_i$ is a bin value.\n",
    "  - Bhattacharyya: $$k(\\mathbf{x},\\mathbf{x}') = \\sum_{i=1}^d \\sqrt{x_i x'_i}$$\n",
    "  - histogram intersection: $$k(\\mathbf{x},\\mathbf{x}') = \\sum_i \\min(x_i, x_i')$$\n",
    "  - $\\chi^2$-RBF: $$k(\\mathbf{x},\\mathbf{x}') = e^{-\\gamma \\chi^2(\\mathbf{x},\\mathbf{x}')}$$\n",
    "    - $\\gamma$ is a inverse bandwidth parameter\n",
    "    - $\\chi^2$ distance: $\\chi^2(\\mathbf{x},\\mathbf{x}') = \\sum_{i=1}^d\\frac{(x_i-x'_i)^2}{\\tfrac{1}{2}(x_i+x'_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Strings**: $\\mathbf{x}$ = \"....\" (strings can be different sizes)\n",
    "  $$k(\\mathbf{x},\\mathbf{x}') = \\sum_{s} w_s \\phi_s(\\mathbf{x})\\phi_s(\\mathbf{x}')$$\n",
    "  - $\\phi_s(\\mathbf{x})$ is the number of times substring $s$ appears in $\\mathbf{x}$.\n",
    "  - $w_s>0$ is a weight.\n",
    "- **Sets**: $\\mathbf{X} = \\{\\mathbf{x}_1,\\cdots, \\mathbf{x}_n\\}$ (sets can be different sizes)\n",
    "  - intersection kernel: $$k(\\mathbf{X},\\mathbf{X}') = 2^{|\\mathbf{X}\\cap \\mathbf{X}'|}$$\n",
    "    - $|\\mathbf{X}\\cap \\mathbf{X}'|$ = number of common elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble methods aim to combine multiple classifiers together to form a better classifier.\n",
    "- Examples:\n",
    "  - **boosting** - training multiple classifiers, each focusing on errors made by previous classifiers.\n",
    "  - **bagging** - training multiple classifiers from random selection of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 AdaBoost - Adaptive Boosting\n",
    "- Base classifier is a \"weak learner\" \n",
    "  - A simple classifier that can be slightly better than random chance (>50%)\n",
    "  - Example: _decision stump classifier_  \n",
    "    - check if feature value is above (or below) a threshold.\n",
    "    - $y = f(x) = \\begin{cases}+1, & x_j \\geq T \\\\ -1, & x_j\\lt T \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Idea:** train weak classifiers sequentially\n",
    "- In each iteration,\n",
    "  - Pick a weak learner $f_t(\\mathbf{x})$ that best carves out the input space.\n",
    "  - The weak learner should focus on data that is misclassified.\n",
    "    - Apply weights to each sample in the training data.\n",
    "    - Higher weights give more priority to difficult samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost loss function\n",
    "- exponential loss\n",
    "  - $L(z_i) = e^{-z_i}$\n",
    "    - $z_i = y_i  f(\\mathbf{x}_i)$\n",
    "  - very sensitive to misclassified outliers.\n",
    "  \n",
    "  \n",
    "  \n",
    "<left><img src=\"imgs/ADA-1.png\" width=\"500px\"/></left> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the list of parameters to try\n",
    "paramgrid = {'n_estimators': array([1, 2, 3, 5, 10, 15, 20, 25, 50, 100, 200, 500, 1000]) }\n",
    "\n",
    "# setup the cross-validation object\n",
    "# (NOTE: using parallelization in GridSearchCV, not in AdaBoost)\n",
    "adacv = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(random_state=4487),\n",
    "                                 paramgrid, cv=5, n_jobs=-1)\n",
    "\n",
    "# run cross-validation (train for each split)\n",
    "adacv.fit(trainX, trainY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the model\n",
    "predY = adacv.predict(testX)\n",
    "\n",
    "# calculate accuracy\n",
    "acc      = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boosting can do feature selection\n",
    "  - each decision stump classifier looks at one feature\n",
    "- One of the original face detection methods (Viola-Jones) used Boosting.\n",
    "  - extract a lot of image features from the face\n",
    "  - during training, Boosting learns which ones are the most useful.  \n",
    "\n",
    "<left><img src=\"imgs/VJ-haar.png\" width=\"500px\"/></left> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- **Ensemble Classifier:**\n",
    "  - Combine the outputs of many \"weak\" classifiers to make a \"strong\" classifier\n",
    "- **Training:**\n",
    "  - In each iteration, \n",
    "    - training data is re-weighted based on whether it is correctly classified or not.\n",
    "    - weak classifier focuses on misclassified data from previous iterations.\n",
    "  - Use cross-validation to pick number of weak learners.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Good generalization performance\n",
    "  - Built-in features selection - decision stump selects one feature at a time.\n",
    "- **Disadvantages:**\n",
    "  - Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "- Simple \"Rule-based\" classifier\n",
    "  - At each node, move down the tree based on that node's criteria.\n",
    "  - leaf node contains the prediction\n",
    "- **Advantage:** can create complex conjunction of rules\n",
    "- **Disadvantage:** easy to overfit by itself\n",
    "  - can fix with bagging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Random Forest Classifier\n",
    "- Use **bagging** to make an ensemble of Decision Tree Classifiers\n",
    "  - for each _Decision Tree Classifier_\n",
    "    - create a new training set by randomly sampling from the training set\n",
    "    - for each split in a tree, select a random subset of features to use\n",
    "- for a test sample, the prediction is aggregated over all trees.\n",
    "\n",
    "<left><img src=\"imgs/RF.jpg\" width=\"500px\"/></left> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn a RF classifier\n",
    "# use 4 trees\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=4, random_state=4487, n_jobs=-1)\n",
    "clf.fit(X3, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {}\n",
    "for i,n in enumerate([5, 10, 50, 100, 1000, 10000]):\n",
    "    clfs[n] = ensemble.RandomForestClassifier(n_estimators=n, random_state=4487, n_jobs=-1)\n",
    "    clfs[n].fit(X3, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the model\n",
    "predY = clfs[1000].predict(testX)\n",
    "\n",
    "# calculate accuracy\n",
    "acc = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Important parameters for cross-validation\n",
    "  - `max_features` - maximum number of features used for each split\n",
    "  - `max_depth` - maximum depth of a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Unbalanced Data\n",
    "- For some classification tasks that data will be unbalanced\n",
    "  - many more examples in one class than the other.\n",
    "- **Example:** detecting credit card fraud\n",
    "  - credit card fraud is rare\n",
    "    - 50 examples of fraud, 5000 examples of legitimate transactions.\n",
    "- **Solution:** apply weights on the classes during training.\n",
    "  - weights are inversely proportional to the class size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfw = svm.SVC(kernel='linear', C=10,  class_weight='balanced')\n",
    "clfw.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Imbalance\n",
    "- In some tasks, errors on certain classes cannot be tolerated.\n",
    "- **Example:** detecting spam vs non-spam\n",
    "  - non-spam should _definitely not_ be marked as spam\n",
    "    - okay to mark some spam as non-spam\n",
    "- Class weighting can be used to make the classifier focus on certain classes\n",
    "  - e.g., weight non-spam class higher than spam class\n",
    "    - classifier will try to correctly classify all non-spam samples, at the expense of making errors on spam samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary (key,value) = (class name, class weight)\n",
    "cw = {0: 0.2, \n",
    "      1:  5}  # class 1 is 25 times more important!\n",
    "\n",
    "clfw = svm.SVC(kernel='linear', C=10,  class_weight=cw)\n",
    "clfw.fit(X, Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Classification Summary\n",
    "\n",
    "- **Classification task**\n",
    "  - Observation $\\mathbf{x}$: typically a real vector of feature values, $\\mathbf{x}\\in\\mathbb{R}^d$.\n",
    "  - Class $y$: from a set of possible classes, e.g., ${\\cal Y} = \\{0,1\\}$\n",
    "  - **Goal:** given an observation $\\mathbf{x}$, predict its class $y$.\n",
    "    \n",
    "<table style=\"font-size:9pt;\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Type</th>\n",
    "<th>Classes</th>\n",
    "<th>Decision function</th>\n",
    "<th>Training</th>\n",
    "<th>Advantages</th>\n",
    "<th>Disadvantages</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Bayes' classifier</td>\n",
    "<td>generative</td>\n",
    "<td>multi-class</td>\n",
    "<td>non-linear</td>\n",
    "<td>estimate class-conditional densities $p(x|y)$ by maximizing likelihood of data.</td>\n",
    "<td>- works well with small amounts of data.<br>- multi-class.<br>- minimum probability of error if probability models are correct.</td>\n",
    "<td>- depends on the data correctly fitting the class-conditional.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>logistic regression</td>\n",
    "<td>discriminative</td>\n",
    "<td>binary</td>\n",
    "<td>linear</td>\n",
    "<td>maximize likelihood of data in $p(y|x)$.</td>\n",
    "<td>- well-calibrated probabilities.<br>- efficient to learn.</td>\n",
    "<td>- linear decision boundary.<br>- sensitive to $C$ parameter.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>support vector machine (SVM)</td>\n",
    "<td>discriminative</td>\n",
    "<td>binary</td>\n",
    "<td>linear</td>\n",
    "<td>maximize the margin (distance between decision surface and closest point).</td>\n",
    "<td>- works well in high-dimension.<br>- good generalization.</td>\n",
    "<td>- linear decision boundary.<br>- sensitive to $C$ parameter.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>kernel SVM</td>\n",
    "<td>discriminative</td>\n",
    "<td>binary</td>\n",
    "<td>non-linear (kernel function)</td>\n",
    "<td>maximize the margin.</td>\n",
    "<td>- non-linear decision boundary.<br>- can be applied to non-vector data using appropriate kernel.</td>\n",
    "<td>- sensitive to kernel function and hyperparameters.<br>\n",
    "- high memory usage for large datasets</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>AdaBoost</td>\n",
    "<td>discriminative</td>\n",
    "<td>binary</td>\n",
    "<td>non-linear (ensemble of weak learners)</td>\n",
    "<td>train successive weak learners to focus on misclassified points.</td>\n",
    "<td>- non-linear decision boundary. can do feature selection.<br>- good generalization.</td>\n",
    "<td>- sensitive to outliers.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Random Forest</td>\n",
    "<td>discriminative</td>\n",
    "<td>multi-class</td>\n",
    "<td>non-linear (ensemble of decision trees)</td>\n",
    "<td>aggregate predictions over several decision trees, trained using different subsets of data.</td>\n",
    "<td>- non-linear decision boundary. can do feature selection.<br>- good generalization.<br>- fast</td>\n",
    "<td>- sensitive to outliers.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<left><img src=\"imgs/CLF-1.png\" width=\"500px\"/></left> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization and Overfitting\n",
    "- Some models have terms to prevent overfitting the training data.\n",
    "  - this can improve _generalization_ to new data.\n",
    "- There is a parameter to control the regularization effect.\n",
    "  - select this parameter using cross-validation on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- *Multiclass classification*\n",
    "  - can use binary classifiers to do multi-class using _1-vs-rest_ formulation.\n",
    "- *Feature normalization*\n",
    "  - normalize each feature dimension so that some feature dimensions with larger ranges do not dominate the optimization process.\n",
    "- *Unbalanced data*\n",
    "  - if more data in one class, then apply weights to each class to balance objectives.\n",
    "- *Class imbalance*\n",
    "  - mistakes on some classes are more critical.\n",
    "  - reweight class to focus classifier on correctly predicting one class at the expense of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning\n",
    "  - Input observation $\\mathbf{x}$, typically a vector in $\\mathbb{R}^d$.\n",
    "  - Output $y \\in \\mathbb{R}$, a real number.\n",
    "- **Goal:** predict output $y$ from input $\\mathbf{x}$.\n",
    "  - i.e., learn the function $y = f(\\mathbf{x})$.\n",
    "  \n",
    "  \n",
    "#### Linear Regression \n",
    "- **1-d case:** the output $y$ is a linear function of input feature $x$\n",
    "  - $y = w * x + b $\n",
    "  - $w$ is the slope, $b$ is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **d-dim case**: the output $y$ is a linear combination of $d$ input variables $x_1, \\cdots, x_d$:\n",
    "  - $y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d$\n",
    "- Equivalently, \n",
    "  - $y = w_0 + \\mathbf{w}^T \\mathbf{x} = w_0 +\\sum_{j=1}^d w_j x_j$\n",
    "  - $\\mathbf{x}\\in\\mathbb{R}^d$ is the vector of input values.\n",
    "  - $\\mathbf{w}\\in\\mathbb{R}^d$ are the weights of the linear function, and $w_0$ is the intercept (bias term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ordinary Least Squares (OLS)\n",
    "- The linear function has form $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + b$.\n",
    "- _How to estimate the parameters $(\\mathbf{w},b)$ from the data?_\n",
    "- Fit the parameters by minimizing the squared prediction error on the training set $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^N$:\n",
    "  $$\\min_{\\mathbf{w},b} \\sum_{i=1}^N (y_i - f(\\mathbf{x}_i))^2 = \\min_{\\mathbf{w},b} \\sum_{i=1}^N (y_i - (\\mathbf{w}^T\\mathbf{x}_i + b))^2$$\n",
    "  \n",
    "  \n",
    "  \n",
    "- closed-form solution: $\\mathbf{w}^* = (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{X}\\mathbf{y}$\n",
    "  - where $\\mathbf{X} = [\\mathbf{x}_1 \\cdots \\mathbf{x}_N]$ is the data matrix\n",
    "  - and $\\mathbf{y} = [y_1, \\cdots, y_N]^T$ is vector of outputs.\n",
    "  - Note: $(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{X}$ is also called the _pseudo-inverse_ of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit using ordinary least squares\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(linX, linY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston_feature_names = [x[0] for x in bostonAttr]\n",
    "boston_df = pd.DataFrame(bostonX, columns=boston_feature_names)\n",
    "\n",
    "tmp=pd.plotting.scatter_matrix(boston_df, c=bostonY, figsize=(9, 9),\n",
    "                              marker='o', hist_kwds={'bins': 20}, s=10,\n",
    "                              alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<left><img src=\"imgs/OLS-1.png\" width=\"500px\"/></left> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrinkage\n",
    "- Add a _regularization_ term to \"shrink\" some linear weights to zero.\n",
    "  - features associated with zero weight are not important since they aren't used to calculate the function output.\n",
    "  - $y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ridge Regression\n",
    "- Add regularization term to OLS:\n",
    "$$\\min_{\\mathbf{w},b} \\alpha ||\\mathbf{w}||^2 + \\sum_{i=1}^N (y_i - f(\\mathbf{x}_i))^2 $$\n",
    "- the first term is the _regularization term_\n",
    "  - $||\\mathbf{w}||^2 = \\sum_{j=1}^d w_j^2$ penalizes large weights.\n",
    "  - $\\alpha$ is the hyperparameter that controls the amount of shrinkage\n",
    "    - larger $\\alpha$ means more shrinkage.\n",
    "    - $\\alpha=0$ is the same as OLS.\n",
    "- the second term is the _data-fit term_\n",
    "  - sum-squared error of the prediction.\n",
    "- Also has a closed-form solution:\n",
    "  - $\\mathbf{w}^* = (\\mathbf{X}\\mathbf{X}^T + \\alpha I)^{-1}\\mathbf{X}\\mathbf{y}$\n",
    "  - (The term \"ridge regression\" comes from the closed-form solution, where a \"ridge\" is added to the diagonal of the covariance matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train RR with cross-validation\n",
    "rr = linear_model.RidgeCV(alphas=alphas, cv=5)\n",
    "rr.fit(trainXn, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "- Which weights are most important?\n",
    "  - negative weights indicate factors that decrease the house price\n",
    "    - *Examples:* LSTAT (having higher percentage of lower status population), DIS (distance to business areas), PTRATIO (higher student-teacher ratio)\n",
    "  - positive weights indicate factors that increase the house price\n",
    "    - *Examples:* RM (having more rooms), RAD (proximity to highways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better shrinkage\n",
    "- With ridge regression, some weights are small but still non-zero.\n",
    "  - these are less important, but somehow still necessary.\n",
    "- To get better shrinkage to zero, we can change the regularization term to encourage more weights to be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LASSO\n",
    "- LASSO = \"Least absolute shrinkage and selection operator\"\n",
    "- keep the same data fit term, but change the regularization term:\n",
    "  - sum of absolute weight values: $\\sum_{j=1}^d|w_j|$\n",
    "  - when a weight is close to 0, the regularization term will force it to be equal to 0.\n",
    "$$\\min_{\\mathbf{w},b} \\alpha \\sum_{j=1}^d|w_j| + \\sum_{i=1}^N (y_i - f(\\mathbf{x}_i))^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with cross-validation (alpha range is determined automatically)\n",
    "las = linear_model.LassoCV()\n",
    "las.fit(trainXn, trainY)\n",
    "\n",
    "MSE = metrics.mean_squared_error(testY, las.predict(testXn))\n",
    "las.alpha_\n",
    "plas.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "- Too many outliers in the data can affect the squared-error term.\n",
    "  - regression function will try to reduce the large prediction error for outliers, at the expense of worse prediction for other points\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<left><img src=\"imgs/OL-1.png\" width=\"500px\"/></left> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 RANSAC\n",
    "- **RAN**dom **SA**mple **C**onsensus\n",
    "  - attempt to robustly fit a regression model in the presence of corrupted data (outliers).\n",
    "  - works with any regression model.\n",
    "- **Idea:**\n",
    "  - split the data into inliers (good data) and outliers (bad data).\n",
    "  - learn the model only from the inliers\n",
    "  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random sampling\n",
    "- Repeat many times...\n",
    "  - randomly sample a subset of points from the data. Typically just enough to learn the regression model\n",
    "  - fit a model to the subset.\n",
    "  - classify all data as inlier or outlier by calculating the residuals (prediction errors) and comparing to a threshold.  The set of inliers is called the _consensus set_.\n",
    "  - save the model with the highest number of inliers.\n",
    "- Finally, use the largest consensus set to learn the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More iterations increases the probability of finding the correct function.\n",
    "  - higher probability to select a subset of points contains all inliers.\n",
    "- Threshold typically set as the median absolute deviation of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use RANSAC model (defaults to linear regression)\n",
    "rlin = linear_model.RANSACRegressor(random_state=1234)\n",
    "rlin.fit(outlinX, outlinY)\n",
    "\n",
    "inlier_mask = rlin.inlier_mask_\n",
    "outlier_mask = logical_not(inlier_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Polynomial regression\n",
    "- p-th order Polynomial function\n",
    "  - $f(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\cdots + w_p x^p$\n",
    "- Collect the terms into a vector\n",
    "  - $f(x) = \\begin{bmatrix}w_0 &w_1 &w_2 &\\cdots &w_p\\end{bmatrix}*\\begin{bmatrix}1\\\\x\\\\x^2 \\\\\\vdots\\\\x^p\\end{bmatrix} = \\mathbf{w}^T \\phi(x)$ \n",
    "  - weight vector $\\mathbf{w} = \\begin{bmatrix}w_0 \\\\w_1 \\\\w_2 \\\\\\vdots \\\\w_p\\end{bmatrix}$; polynomial feature vector: $\\phi(x) = \\begin{bmatrix}1\\\\x\\\\x^2 \\\\\\vdots\\\\x^p\\end{bmatrix}$\n",
    "- Now it's a linear function, so we can use the same linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/PR-1.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the pipeline\n",
    "#  each entry is a tuple with the name and transformer (implements fit, transform)\n",
    "#  the last entry should be a model (implements fit)\n",
    "polylin = pipeline.Pipeline([\n",
    "        ('polyfeats', preprocessing.PolynomialFeatures(degree=1)), \n",
    "        ('linreg',    linear_model.LinearRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters for grid search\n",
    "# the parameters in each stage are named: <stage>__<parameter> \n",
    "paramgrid = {\n",
    "    \"polyfeats__degree\": array([1, 2, 3, 4, 5, 6]),\n",
    "}\n",
    "\n",
    "# do the cross-validdation search \n",
    "plincv = model_selection.GridSearchCV(polylin, paramgrid, cv=5, n_jobs=-1,\n",
    "                                      scoring='neg_mean_squared_error')\n",
    "plincv.fit(bostonX, bostonY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Kernel Ridge Regression\n",
    "- Apply _kernel trick_ to ridge regression\n",
    "  - turn linear regression into non-linear regression\n",
    "- Closed form solution:\n",
    "  - for an input point $\\mathbf{x}_*$,\n",
    "    - prediction: $y_* = \\mathbf{k}_* (\\mathbf{K} + \\alpha I) ^{-1} \\mathbf{y}$\n",
    "      - $\\mathbf{K}$ - the kernel matrix ($N \\times N$)\n",
    "      - $\\mathbf{k}_*$ - vector containing the kernel values between $\\mathbf{x}_*$ and all training points $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for cross-validation\n",
    "paramgrid = {'alpha': logspace(-3,3,10),\n",
    "          'gamma': logspace(-3,3,10)}\n",
    "\n",
    "# do cross-validation\n",
    "krrcv = model_selection.GridSearchCV(\n",
    "  kernel_ridge.KernelRidge(kernel='rbf'),  # estimator\n",
    "  paramgrid,                              # parameters to try\n",
    "  scoring='neg_mean_squared_error',       # score function\n",
    "  cv=5,                                   # number of folds\n",
    "  n_jobs=-1, verbose=True)\n",
    "krrcv.fit(bostonX, bostonY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for cross-validation\n",
    "paramgrid = {'alpha': logspace(-3,3,10),\n",
    "          'degree': [2,3,4]}\n",
    "\n",
    "# do cross-validation\n",
    "krrcv = model_selection.GridSearchCV(\n",
    "  kernel_ridge.KernelRidge(kernel='poly'),  # estimator\n",
    "  paramgrid,                              # parameters to try\n",
    "  scoring='neg_mean_squared_error',       # score function\n",
    "  cv=5,                                   # number of folds\n",
    "  n_jobs=-1, verbose=True)\n",
    "krrcv.fit(bostonX, bostonY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Support Vector Regression (SVR)\n",
    "- Borrow ideas from classification\n",
    "  - Suppose we form a \"band\" of width $\\epsilon$ around the function:\n",
    "    - if a point is inside, then it is \"correctly\" predicted\n",
    "    - if a point is outside, then it is incorrectly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allow some points to be outside the \"tube\".\n",
    "  - penalty of point outside tube is controlled by $C$ parameter.\n",
    "- SVR objective function:\n",
    "$$\\min_{\\mathbf{w},b} \\sum_{i=1}^N \\left|y_i - (\\mathbf{w}^T\\mathbf{x}_i+b)\\right|_{\\epsilon} + \\frac{1}{C}||\\mathbf{w}||^2$$\n",
    "\n",
    "- epsilon-insensitive error:\n",
    "  - $\\left|z\\right|_{\\epsilon} = \\begin{cases} 0, & |z|\\leq \\epsilon \\\\ |z|-\\epsilon, & |z|\\gt \\epsilon\\end{cases}$\n",
    "- Similar to SVM classifier, the points on the band will be the _support vectors_ that define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 11.5\n",
    "svr = svm.SVR(C=1000, kernel='linear', epsilon=epsilon)\n",
    "svr.fit(linX, linY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVR\n",
    "- Support vector regression can also be kernelized similar to SVM\n",
    "  - turn linear regression to non-linear regression\n",
    "- Polynomial Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for cross-validation\n",
    "paramgrid = {'C':       logspace(-3,3,10),\n",
    "             'gamma':   logspace(-3,3,10),\n",
    "             'epsilon': logspace(-2,2,10)}\n",
    "\n",
    "# do cross-validation \n",
    "svrcv = model_selection.GridSearchCV(\n",
    "    svm.SVR(kernel='rbf'),  # estimator\n",
    "    paramgrid,                     # parameters to try\n",
    "    scoring='neg_mean_squared_error',  # score function\n",
    "    cv=5, \n",
    "    n_jobs=-1, verbose=1)                # show progress\n",
    "svrcv.fit(bostonX, bostonY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for cross-validation\n",
    "paramgrid = {'C':       logspace(-3,3,10),\n",
    "             'degree':  [2.3.4],\n",
    "             'epsilon': logspace(-2,2,10)}\n",
    "\n",
    "# do cross-validation \n",
    "svrcv = model_selection.GridSearchCV(\n",
    "    svm.SVR(kernel='poly'),  # estimator\n",
    "    paramgrid,                     # parameters to try\n",
    "    scoring='neg_mean_squared_error',  # score function\n",
    "    cv=5, \n",
    "    n_jobs=-1, verbose=1)                # show progress\n",
    "svrcv.fit(bostonX, bostonY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Random Forest Regression\n",
    "- Similar to Random Forest Classifier\n",
    "  - Average predictions over many Decision Trees\n",
    "    - Each decision tree sees a random sampling of the Training set\n",
    "    - Each split in the decision tree uses a random subset of features\n",
    "    - Leaf node of tree contains the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestRegressor(n_estimators=4, random_state=4487, n_jobs=-1)\n",
    "rf.fit(polyX, polyY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/RFC-1.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for cross-validation\n",
    "paramgrid = {'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15]),\n",
    "            }\n",
    "\n",
    "# do cross-validation\n",
    "rfcv = model_selection.GridSearchCV(\n",
    "    ensemble.RandomForestRegressor(n_estimators=100, random_state=4487),  # estimator\n",
    "    paramgrid,                     # parameters to try\n",
    "    scoring='neg_mean_squared_error',  # score function\n",
    "    cv=5,\n",
    "    n_jobs=-1, verbose=True\n",
    ")\n",
    "rfcv.fit(bostonX, bostonY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Regression Summary\n",
    "- **Goal:** predict output $y\\in\\mathbb{R}$ from input $\\mathbf{x}\\in\\mathbb{R}^d$.\n",
    "  - i.e., learn the function $y = f(\\mathbf{x})$.\n",
    "\n",
    "<table style=\"font-size:9pt;\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Function</th>\n",
    "<th>Training</th>\n",
    "<th>Advantages</th>\n",
    "<th>Disadvantages</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Ordinary Least Squares</td>\n",
    "<td>linear</td>\n",
    "<td>minimize square error between observation and predicted output.</td>\n",
    "<td>- closed-form solution.</td>\n",
    "<td>- sensitive to outliers and overfitting.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ridge regression</td>\n",
    "<td>linear</td>\n",
    "<td>minimize squared error with $||w||^2$ regularization term.</td>\n",
    "<td>- closed-form solution;<br>- shrinkage to prevent overfitting.</td>\n",
    "<td>- sensitive to outliers.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LASSO</td>\n",
    "<td>linear</td>\n",
    "<td>minimize squared error with $\\sum_{j=1}^d|w_j|$ regularization term.</td>\n",
    "<td>- feature selection (by forcing weights to 0)</td>\n",
    "<td>- sensitive to outliers.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RANSAC</td>\n",
    "<td>same as the base model</td>\n",
    "<td>randomly sample subset of training data and fit model; keep model with most inliers.</td>\n",
    "<td>- ignores outliers.</td>\n",
    "<td>- requires enough iterations to find good consensus set.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>kernel ridge regression</td>\n",
    "<td>non-linear (kernel function)</td>\n",
    "<td>apply \"kernel trick\" to ridge regression.</td>\n",
    "<td>- non-linear regression. <br>- Closed-form solution.</td>\n",
    "<td>- requires calculating kernel matrix $O(N^2)$.<br>- cross-validation to select hyperparameters.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>kernel support vector regression</td>\n",
    "<td>non-linear (kernel function)</td>\n",
    "<td>minimize squared error, insensitive to epsilon-error.</td>\n",
    "<td>- non-linear regression.<br>- faster predictions than kernel ridge regression.</td>\n",
    "<td>- requires calculating kernel matrix $O(N^2)$.<br>- iterative solution (slow).<br>- cross-validation to select hyperparameters.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>random forest regression</td>\n",
    "<td>non-linear (ensemble)</td>\n",
    "<td>aggregate predictions from decision trees.</td>\n",
    "<td>- non-linear regression.<br>- fast predictions.</td>\n",
    "<td>- predicts step-wise function.<br>- cannot learn a completely smooth function.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Things\n",
    "- _Feature normalization_\n",
    "  - feature normalization is typically required for regression methods with regularization.\n",
    "  - makes ordering of weights more interpretable (LASSO, RR).\n",
    "- _Output transformations_\n",
    "  - sometimes the output values $y$ have a large dynamic range (e.g., $10^{-1}$ to $10^5$).\n",
    "    - large output values will have large error, which will dominate the training error.\n",
    "  - in this case, it is better to transform the output values using the logarithm function.\n",
    "    - $\\hat{y} = \\log_{10} (y)$\n",
    "  - For example, see the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each data point is a vector $\\mathbf{x}\\in\\mathbb{R}^d$.\n",
    "- Data is set of vectors $\\{\\mathbf{x}_1,\\cdots,\\mathbf{x}_n\\}$\n",
    "- **Goal:** group similar data together.\n",
    "  - groups are also called clusters.\n",
    "  - each data point is assigned with a cluster index ($y\\in\\{1,\\cdots,K\\}$)\n",
    "    - $K$ is the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K-Means Clustering\n",
    "- **Idea:**\n",
    "  - there are $K$ clusters.\n",
    "  - each cluster is represented by a _cluster center_.\n",
    "    - $\\mathbf{c}_j \\in \\mathbf{R}^d$, $j\\in\\{1,\\cdots,K\\}$\n",
    "  - assign each data point to the closest cluster center.\n",
    "    - according to Euclidean distance: $\\|\\mathbf{x}_i - \\mathbf{c}_j\\|$\n",
    "    \n",
    "- _How to pick the cluster centers?_\n",
    "  - Assume there are $K$ clusters\n",
    "  - Pick the cluster centers that minimize the squared distance to all its cluster members.\n",
    "$$ \\min_{\\mathbf{c}_1,\\cdots,\\mathbf{c}_K} \\sum_{i=1}^n || \\mathbf{x}_i - \\mathbf{c}_{z_i}||^2$$\n",
    "\n",
    "  - where $z_i$ is the index of the closest cluster center to $\\mathbf{x}_i$.\n",
    "      - $z_i = \\mathop{\\mathrm{argmin}}_{j=\\{1,\\cdots,K\\}} ||\\mathbf{x}_i - \\mathbf{c}_j||$   \n",
    "      - i.e., the assignment of point $\\mathbf{x}_i$ to its closest cluster.\n",
    "      \n",
    "      \n",
    "      \n",
    "- Solution: \n",
    "  - if the assignments $\\{z_i\\}$ are known...\n",
    "    - let $C_j$ be the set of points assigned to cluster $j$\n",
    "      - $C_j = \\{ \\mathbf{x}_i | z_i=j\\}$\n",
    "    - cluster center is the mean of the points in the cluster\n",
    "      - $\\mathbf{c}_j = \\frac{1}{|C_j|}\\sum_{\\mathbf{x}_i \\in C_j} \\mathbf{x}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means Algorithm\n",
    "- Pick initial cluster centers\n",
    "- Repeat:\n",
    "  - 1) calculate assignment $z_i$ for each point $\\mathbf{x}_i$: closest cluster center using Euclidean distance.\n",
    "  - 2) calculate cluster center $\\mathbf{c}_j$ as average of points assigned to cluster $j$.\n",
    "- This procedure will converge eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Note\n",
    "- The final result depends on the initial cluster centers!\n",
    "  - Some bad initializations will yield poor clustering results!\n",
    "  - (Technically, there are multiple local minimums in the objective function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Solution:**\n",
    "  - Try several times using different initializations.\n",
    "  - Pick the answer with lowest objective score.\n",
    "- In scikit-learn,\n",
    "  - automatically uses multiple random initializations.\n",
    "  - also uses a smart initialization method called \"k-means++\"\n",
    "  - can run initialization runs in parallel (`n_jobs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means with 3 clusters\n",
    "# (automatically does 10 random initializations)\n",
    "km = cluster.KMeans(n_clusters=3, random_state=4487, n_jobs=-1)\n",
    "Yp = km.fit_predict(X)   # cluster data, and return labels\n",
    "\n",
    "cc = km.cluster_centers_   # the cluster centers\n",
    "cl = km.labels_            # labels also stored here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Circular clusters\n",
    "- One problem with K-means is that it assumes that each cluster has a circular shape.\n",
    "  - based on Euclidean distance to each center\n",
    "  - Kmeans cannot handle skewed (elliptical) clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Gaussian mixture model (GMM)\n",
    "- A multivariate Gaussian can model a cluster with an elliptical shape.\n",
    "  - the ellipse shape is controlled by the covariance matrix of the Gaussian\n",
    "  - the location of the cluster is controlled by the mean.\n",
    "- Gaussian mixture model is a weighted sum of Gaussians\n",
    "$$ p(\\mathbf{x}) = \\sum_{j=1}^K \\pi_j N(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)$$\n",
    "  - Each Gaussian represents one elliptical cluster\n",
    "    - $\\boldsymbol{\\mu}_j$ is the mean of the j-th Gaussian. (the location)\n",
    "    - $\\boldsymbol{\\Sigma}_j$ is the covariance matrix of the j-th Gaussian. (the ellipse shape)\n",
    "    - $\\pi_j$ is the prior weight of the j-th Gaussian. (how likely is this cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with GMMs\n",
    "- Using the data, learn a GMM using maximum likelihood estimation:\n",
    "  $$\\max_{\\pi,\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}} \\sum_{i=1}^N \\log \\sum_{j=1}^K \\pi_j N(\\mathbf{x}_i|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Results in an algorithm similar to K-means:\n",
    "  - 1) Calculate cluster membership\n",
    "    - uses \"soft\" assignment - a data point can have a fractional contribution to different clusters.\n",
    "    - contribution of point $i$ to cluster $j$\n",
    "      - $z_{ij} = p(z_i=j | \\mathbf{x}_i)$\n",
    "  - 2) Update each Gaussian cluster (mean, covariance, and weight)\n",
    "    - uses \"soft\" weighting\n",
    "      - \"soft\" count of points in cluster j: $N_j = \\sum_{i=1}^N z_{ij}$\n",
    "      - weight: $\\pi_j = N_j / N$\n",
    "      - mean: $\\boldsymbol{\\mu}_j = \\frac{1}{N_j} \\sum_{i=1}^N z_{ij} \\mathbf{x}_i$\n",
    "      - variance: $\\Sigma_j = \\frac{1}{N_j} \\sum_{i=1}^N z_{ij} (\\mathbf{x}_i - \\boldsymbol{\\mu}_j)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a GMM\n",
    "gmm = mixture.GaussianMixture(n_components=4, random_state=4487, n_init=10)\n",
    "\n",
    "gmm.fit(X)\n",
    "Y = gmm.predict(X)\n",
    "\n",
    "cc = gmm.means_     # the cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance matrix\n",
    "- The covariance matrix is a $d \\times d$ matrix.\n",
    "  $$ \\begin{bmatrix}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}$$\n",
    "- For high-dimensional data, it can be very large.\n",
    "  - requires a lot of data to learn effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Solution:\n",
    "  - use *diagonal* covariance matrices ($d$ parameters):\n",
    "  $$ \\begin{bmatrix}a_{11} & 0 & 0 \\\\ 0 & a_{22} & 0 \\\\ 0 & 0 & a_{33} \\end{bmatrix}$$\n",
    "    - Axes of ellipses will be aligned with the axes.\n",
    "  - use *spherical* covariance matrices (1 parameter)\n",
    "    $$ \\begin{bmatrix}a & 0 & 0 \\\\ 0 & a & 0 \\\\ 0 & 0 & a \\end{bmatrix}$$\n",
    "    - Clusters will be circular (similar to K-means)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full covariance (d*d parameters)\n",
    "gmmf = mixture.GaussianMixture(n_components=2, covariance_type='full',\n",
    "                               random_state=4487, n_init=10)\n",
    "gmmf.fit(X)\n",
    "\n",
    "# diagonal convariance (d parameters)\n",
    "gmmd = mixture.GaussianMixture(n_components=2, covariance_type='diag',\n",
    "                               random_state=4487, n_init=10)\n",
    "gmmd.fit(X)\n",
    "\n",
    "# spherical covariance (1 parameter)\n",
    "gmms = mixture.GaussianMixture(n_components=2, covariance_type='spherical',\n",
    "                               random_state=4487, n_init=10)\n",
    "gmms.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/GMM-1.png\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Dirichlet Process GMM\n",
    "  - GMM is extended to automatically select the value of $K$\n",
    "    - use a \"Dirichlet Process\" to model $K$ as a random variable.\n",
    "  - _concentration_ parameter $\\alpha$ controls the range of $K$ values that are preferred\n",
    "    - higher values encourage more clusters\n",
    "    - lower values encourage less clusters\n",
    "    - expected number of clusers is $\\alpha \\log N$, where $N$ is the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = concentration parameter\n",
    "# n_components = the max number of components to consider\n",
    "dpgmm = mixture.BayesianGaussianMixture(covariance_type='full', \n",
    "                   weight_concentration_prior=1, \n",
    "                   n_components=5, max_iter=100, random_state=4487)\n",
    "dpgmm.fit(X)\n",
    "Y = dpgmm.predict(X)\n",
    "cl = unique(Y)         # find active clusters\n",
    "newK = len(cl)         # number of clusters\n",
    "cc = dpgmm.means_[cl]  # get means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Mean-shift algorithm\n",
    "- Clustering algorithm that also automatically selects the number of clusters.\n",
    "- **Idea:** iteratively shift towards the largest concentration of points.\n",
    "  - start from an initial point $\\mathbf{x}$ (e.g., one of the data points).\n",
    "  - repeat until $\\mathbf{x}$ is unchanged:\n",
    "    - 1) find the nearest neighbors to $\\mathbf{x}$ within some radius (bandwidth)\n",
    "    - 2) set $\\mathbf{x}$ to be the mean of the neighbor points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the clusters\n",
    "- Run the mean-shift algorithm for many initial points $\\{x_i\\}$.\n",
    "  - the set of converged points contain the cluster centers.\n",
    "    - need to remove the duplicate centers.\n",
    "  - data points that converge to the same center belong to the same cluster.\n",
    "  - different initializations can run in parallel (`n_jobs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_seeding=True -- coarsely uses data points as initial points\n",
    "ms = cluster.MeanShift(bandwidth=5, bin_seeding=True, n_jobs=-1)\n",
    "Y = ms.fit_predict(X)\n",
    "\n",
    "cc = ms.cluster_centers_  # cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters\n",
    "- Number of clusters is implicitly controlled by the bandwidth (radius of the nearest-neighbors)\n",
    "  - larger bandwidth creates less clusters\n",
    "    - focuses on global large groups\n",
    "  - smaller bandwidth creates more clusters\n",
    "    - focuses on local groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-compact clusters\n",
    "- K-means, GMM, and Mean-Shift assume that all clusters are compact.\n",
    "  - i.e., circles or ellipses\n",
    "- What about clusters of other shapes?\n",
    "  - e.g., clusters not defined by compact distance to a \"center\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Spectral Clustering\n",
    "- Estimate the clusters using the pair-wise affinity between points.\n",
    "- Affinity (similarity) between points \n",
    "  - kernel function: $k(\\mathbf{x}_i,\\mathbf{x}_j)$ -- RBF kernel\n",
    "  - number of nearest neighbors within a radius (bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = cluster.SpectralClustering(n_clusters=4, affinity='rbf', gamma=1.0, assign_labels='discretize', n_jobs=-1)\n",
    "Y = sc.fit_predict(X)  # cluster and also return the labels Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering\n",
    "- **Idea:** clustering with a graph formulation\n",
    "  - each data point is a node in a graph\n",
    "  - edge weight between two nodes is the affinity $k(\\mathbf{x}_i,\\mathbf{x}_j)$\n",
    "    - (darker colors indicate stronger weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/SC-1.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity to gamma\n",
    "- gamma controls which structures are important\n",
    "  - small gamma - far away points are still considered similar\n",
    "  - large gamma - close points are not considered similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 DBSCAN\n",
    "  - \"Density-Based Spatial Clustering of Applications with Noise\"\n",
    "    - Assumption: clusters are regions of high density of points separated by areas of low density.\n",
    "    - Algorithm Idea: \n",
    "      - Find a _core_ point of high density.\n",
    "      - Recursively label the neighbors as core points.\n",
    "      - Neighbors that are not core samples are called _boundary_ or _non-core_ points.\n",
    "      - Points that are not boundary and not core points are _outliers_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define two parameters:\n",
    "  - `eps`:  the maximum distance to be considered a neighbor.\n",
    "  - `min_samples`: the minimum number of neighbors (including point itself) to be considered a core sample.\n",
    "<center><img src=\"imgs/dbscan.jpg\", width ='600px'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = the max distance to be considered a neighbor\n",
    "# min_samples = min number of neighbors to be a core sample\n",
    "dbs = cluster.DBSCAN(eps=0.5, min_samples=5, n_jobs=-1)\n",
    "Y = dbs.fit_predict(X)\n",
    "\n",
    "# labels: -1 means outlier\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Effect of `eps`\n",
    "  - smaller `eps` - high density required to make a single cluster\n",
    "  - larger `eps` - encourages collapsing of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Effect of `min_samples`\n",
    "  - smaller `min_samples` -  encourages forming small clusters\n",
    "  - larger `min_samples` - forms clusters only in very high-density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Clustering Summary\n",
    "- **Goal:** given set of input vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$, with $\\mathbf{x}_i\\in\\mathbb{R}^d$, group similar $x_i$ together into clusters.\n",
    "  - estimate a cluster center, which represents the data points in that cluster.\n",
    "  - predict the cluster for a new data point.\n",
    "  \n",
    "<table style=\"font-size:9pt;\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Cluster Shape</th>\n",
    "<th>Principle</th>\n",
    "<th>Advantages</th>\n",
    "<th>Disadvantages</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>K-Means</td>\n",
    "<td>circular</td>\n",
    "<td>minimize distance to cluster center</td>\n",
    "<td>- scalable (MiniBatchKMeans)</td>\n",
    "<td>- sensitive to initialization; could get bad solutions due to local minima.<br>- need to choose K.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Gaussian Mixture Model</td>\n",
    "<td>elliptical</td>\n",
    "<td>maximum likelihood</td>\n",
    "<td>- elliptical cluster shapes.</td>\n",
    "<td>- sensitive to initialization; could get bad solutions due to local minima.<br>- need to choose K.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Dirichlet Process GMM</td>\n",
    "<td>elliptical</td>\n",
    "<td>maximum likelihood</td>\n",
    "<td>- automatically selects K via concentration parameter.</td>\n",
    "<td>- can be slow.<br>- sensitive to initialization; could get bad solutions due to local minima.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Mean-Shift</td>\n",
    "<td>concentrated compact</td>\n",
    "<td>move towards local mean</td>\n",
    "<td>- automatically selects K via bandwidth parameter.</td>\n",
    "<td>- can be slow.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Spectral clustering</td>\n",
    "<td>irregular shapes</td>\n",
    "<td>graph-based</td>\n",
    "<td>- can handle clusters of any shape, as long as connected.</td>\n",
    "<td>- need to choose K.<br>- cannot assign novel points to a cluster.<br>- can be slow (kernel matrix)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DBSCAN</td>\n",
    "<td>irregular shapes</td>\n",
    "<td>density-based</td>\n",
    "<td>- can handle clusters of any shape, as densely sampled.<br>- can detect outliers</td>\n",
    "<td>- sensitive to parameters<br>- cannot assign novel points to a cluster.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Things\n",
    "- _Feature normalization_\n",
    "  - feature normalization is typically required clustering.\n",
    "  - e.g., algorithms based on Euclidean distance (Kmeans, Mean-Shift, Spectral Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Goal:** Transform high-dimensional vectors into low-dimensional vectors.\n",
    "  - Dimensions in the low-dim data represent co-occuring features in high-dim data.\n",
    "  - Dimensions in the low-dim data may have semantic meaning.\n",
    "- **For example:** document analysis\n",
    "  - high-dim: bag-of-word vectors of documents\n",
    "  - low-dim: each dimension represents similarity to a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasons for Dimensionality Reduction\n",
    "- Preprocessing - make the dataset easier to use\n",
    "- Reduce computational cost of running machine learning algorithms\n",
    "- Remove noise\n",
    "- Make the results easier to understand (visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Linear Dimensionality Reduction for Vectors\n",
    "- Project the original data onto a lower-dimensional hyperplane (e.g., line, plane).\n",
    "  - I.e, Move and rotate the coordinate axis of the data\n",
    "- Represent the data with coordinates in the new component space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equivalently, approximate the data point $\\mathbf{x}$ as a linear combination of basis vectors (components) in the original space.\n",
    "  - original data point $\\mathbf{x}\\in\\mathbb{R}^d$\n",
    "  - approximation: $\\hat{\\mathbf{x}} = \\sum_{j=1}^p w_j \\mathbf{v}_j$\n",
    "    - $\\mathbf{v}_j\\in\\mathbb{R}^d$ is a basis vector and $w_j\\in\\mathbb{R}$ the corresponding weight.\n",
    "  - the data point $\\mathbf{x}$ is then represented its corresponding weights\n",
    "    - $\\mathbf{w} = [w_1,\\cdots,w_P] \\in \\mathbb{R}^p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Several methods for linear dimensionality reduction.\n",
    "- **Differences:**\n",
    "  - goal (reconstruction vs classification)\n",
    "  - unsupervised vs. supervised\n",
    "  - constraints on the basis vectors and the weights.\n",
    "  - reconstruction error criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Principal Component Analysis (PCA)\n",
    "- Unsupervised method\n",
    "- **Goal:** preserve the variance of the data as much as possible\n",
    "  - choose basis vectors along the maximum variance (longest extent) of the data.\n",
    "  - the basis vectors are called _principal components_ (PC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Goal:** Equivalently, minimize the reconstruction error over all the data points $\\{\\mathbf{x}_i\\}_{i=1}^N$.\n",
    "  - reconstruction: $\\hat{\\mathbf{x}}_i = \\sum_{j=1}^p w_{i,j} \\mathbf{v}_j$\n",
    "  $$\\min_{\\mathbf{}w, \\mathbf{v}} \\sum_{i=1}^N ||\\mathbf{x}_i - \\hat{\\mathbf{x}}_i||^2$$\n",
    "  - _constraint:_ principal components $\\mathbf{v}_j$ are orthogonal (perpendicular) to each other.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA algorithm\n",
    "- 1) subtract the mean of the data\n",
    "- 2) the first PC $v_1$ is the direction that explains the most variance of the data.\n",
    "- 3) the second PC $v_2$ is the direction perpendicular to $v_1$ that explains the most variance.\n",
    "- 4) the third PC $v_3$ is the direction perpendicular to $\\{v_1,v_2\\}$ that explains the most variance.\n",
    "- 5) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "W   = pca.fit_transform(X)  # returns the coefficients\n",
    "Wt = pca.transform(testX)  # use the pca model to transform the test set\n",
    "\n",
    "v = pca.components_  # the principal component vector\n",
    "m = pca.mean_        # the data mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/PCA-1.png\", width ='600px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose the number of principal components?\n",
    "- Two methods to set the number of components $p$:\n",
    "  - preserve some percentage of the variance (e.g., 95%).\n",
    "  - whatever works well for our final task (e.g., classification, regression).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained variance\n",
    "- each PC explains a percentage of the original data\n",
    "  - this is called the _explained variance_.\n",
    "  - PCs are already sorted by explained variance from highest to lowest\n",
    "- pick the number of PCs to get a certain percentage of explained variance\n",
    "  - typically 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev    = pca.explained_variance_ratio_  # variance explained by each component\n",
    "cumev = cumsum(ev)                     # cumulative explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Random Projections\n",
    "- If the data is very high-dimensional, then it might take too many calculations to do PCA.\n",
    "  - Complexity: $O(d k^2)$, $d$ is the dimension, $k$ is the number of components\n",
    "- Do we really need to estimate the principal components to reduce the dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Solution:**\n",
    "  - We can generate random basis vectors and use those.\n",
    "    - Each entry of $\\mathbf{v}_j$ sampled from a Gaussian.\n",
    "  - This will save a lot of time.\n",
    "  - Random Projections can reduce computation at the expense of losing some accuracy in the points (adding noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the digits data with Random Projection\n",
    "rp = random_projection.GaussianRandomProjection(n_components=2, random_state=4487)\n",
    "Wrp = rp.fit_transform(trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay, but is it good?\n",
    "  - One way to measure \"goodness\" is to see if the structure of the data is preserved.\n",
    "  - In other words, are distances between points preserved in the transformed data?\n",
    "<center><img src=\"imgs/rp.jpg\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Answer:**\n",
    "  - Yes!\n",
    "  - According to the _Johnson-Lindenstrauss lemma_, carefully selecting the distribution of the random projection matrices will preserve the preserve the pairwise distances between any two samples of the dataset, within some error _epsilon_.\n",
    "    - $ (1-\\epsilon)\\| \\mathbf{x}_i - \\mathbf{x}_j\\|^2 < \\|\\mathbf{w}_i - \\mathbf{w}_j\\|^2 < (1+\\epsilon) \\| \\mathbf{x}_i - \\mathbf{x}_j\\|^2$\n",
    "    - the minimum reduced dimension $p$ to gaurantee $\\epsilon$ error depends on the number of samples.\n",
    "      - (actually, this is fairly conservative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Sparse Random Projection\n",
    "- More computation can be saved by using a _sparse_ random projection matrix\n",
    "  - \"sparse\" means that many entries in the basis vector are zero, so we can ignore those entries when multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the digits data with Random Projection\n",
    "srp = random_projection.SparseRandomProjection(n_components=500, random_state=4487)\n",
    "Wsrp = srp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "- Suppose we have data for the below classification problem...\n",
    "- We want to reduce the data to 1 dimension using PCA.\n",
    "  - What is the first PC?\n",
    "    \n",
    "#### Answer\n",
    "- first PC is along the direction of most variance.\n",
    "  - collapses the two classes together!    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with Unsupervised Methods\n",
    "- If our end goal is classification, preserving the variance sometimes won't help!\n",
    "  - PCA doesn't consider which class the data belongs to.\n",
    "  - When the \"classification\" signal is less than the \"noise\", PCA will make classification more difficult.\n",
    "  \n",
    "  \n",
    "  \n",
    "### 5.1.4 Fisher's Linear Discriminant (FLD)\n",
    "- Supervised dimensionality reduction\n",
    "- Also called _\"Linear Discriminant Analysis\"_ (LDA)\n",
    "- **Goal:** find a lower-dim space so as to minimize the class overlap (or maximize the class separation).\n",
    "  - data from each class is modeled as a Gaussian.\n",
    "  - requires the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of FLD projection (using LDA name)\n",
    "fld = discriminant_analysis.LinearDiscriminantAnalysis(n_components=1)\n",
    "W  = fld.fit_transform(X, Y)\n",
    "\n",
    "v = fld.coef_   # the basis vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Linear Dimensionality Reduction for Tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Latent Semantic Analysis (LSA)\n",
    "- Also called _Latent Semantic Indexing_\n",
    "- Consider a bag-of-word representation (e.g., TF, TF-IDF)\n",
    "  - document vector $\\mathbf{x}_i$ \n",
    "  - $x_{i,j}$ is the frequency of word $j$ in document $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approximate each document vector as a weighted sum of topic vectors.\n",
    "  - $\\hat{\\mathbf{x}} = \\sum_{n=1}^p w_p \\mathbf{v}_p$\n",
    "  - Topic vector $\\mathbf{v}_p$ contains co-occuring words.\n",
    "    - corresponds to a particular _topic_ or _theme_.\n",
    "  - Weight $w_p$ represents similarity of the document to the p-th topic.\n",
    "- Objective: \n",
    "  - minimize the squared reconstruction error (Similar to PCA):\n",
    "  - $\\min_{\\mathbf{v},\\mathbf{w}} \\sum_i ||\\mathbf{x}_i - \\hat{\\mathbf{x}}_i||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Represent each document by its topic weights.\n",
    "  - Apply other machine learning algorithms...\n",
    "- **Advantage:**   \n",
    "  - Finds relations between terms (synonymy and polysemy).\n",
    "  - distances/similarities are now comparing topics rather than words.\n",
    "    - higher-level semantic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = decomposition.TruncatedSVD(n_components=5, random_state=4487)\n",
    "Wlsa = lsa.fit_transform(Xtf) \n",
    "\n",
    "# components\n",
    "V = lsa.components_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vectors\n",
    "- topic vectors contain frequent co-occuring words\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/TV-1.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with LSA\n",
    "- In the topic vector, the \"frequency\" of a word can be negative!\n",
    "  - Doesn't really make sense for document bag-of-words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with LSA\n",
    "- The weights for each topic can be negative!\n",
    "  - Topics should only be \"additive\"\n",
    "    - Topics should increase probability of some topic-related words, but not decrease probability of other words.\n",
    "    - It doesn't make sense to \"remove\" a topic using a negative topic weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Non-negative Matrix Factorization (NMF)\n",
    "- **Solution:** constrain the topic vector and weights to be non-negative.\n",
    "- Similar to LSA\n",
    "  - Approximate each document vector as a weighted sum of topic vectors.\n",
    "    - $\\hat{\\mathbf{x}}_j = \\sum_{n=1}^p w_p \\mathbf{v}_p$\n",
    "    - But now, each entry of topic vector $\\mathbf{v}_p\\geq 0$ and topic weight $w_p\\geq 0$\n",
    "  - Objective: minimize the squared reconstruction error\n",
    "    - $\\min_{\\mathbf{v},\\mathbf{w}} \\sum_j ||\\mathbf{x}_j - \\hat{\\mathbf{x}}_j||^2$\n",
    "    - subject to the non-negative constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "nmf = decomposition.NMF(n_components=5)\n",
    "Wnmf = nmf.fit_transform(Xtf) \n",
    "\n",
    "# components\n",
    "V = nmf.components_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparseness\n",
    "- For NMF representation, most topic weights for a document are zero.\n",
    "  - this is called a _sparse_ representation.\n",
    "  - each document is only composed of a few topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with NMF\n",
    "- _While the weights and component vectors are non-negative, NMF does not enforce them to be  probabilities._\n",
    "- TF/TFIDF is a probabilistic model of words in a document\n",
    "  - the vector of probabilities sums to 1\n",
    "  - probabilities are between 0 and 1\n",
    "- The NMF components and weights are difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Latent Dirichlet Allocation (LDA)\n",
    "- Use a generative probabilistic framework to model topics and documents.\n",
    "- A document is composed of a mixture of topics.\n",
    "  - Each topic has its own distribution of words (topic vector $\\boldsymbol{\\beta}_k$).\n",
    "    - Topic vectors are shared among documents.\n",
    "  - Each document has its own topic weighting.  \n",
    "    - The d-th document: $\\hat{\\mathbf{x}}_d = \\sum_{k=1}^K \\theta_{d,k} \\boldsymbol{\\beta}_k$\n",
    "      - $\\theta_{d,k}$ is the probability of the k-th topic occurring in the d-th document.\n",
    "      - $\\boldsymbol{\\beta}_k$ is the topic vector for the k-th topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA graphical model\n",
    "- Each node is a random variable.\n",
    "- Plates (boxes) denote a vector of random variables.\n",
    "  - The size is given in the bottom-right corner.\n",
    "  \n",
    "<center><img src=\"imgs/LDA.jpg\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA generative model\n",
    "  1. For each topic $k = 1\\ldots K$:\n",
    "    1. Sample the topic vector $\\boldsymbol{\\beta}_k$ from a Dirichlet distribution.\n",
    "  2. For each document $d=1\\ldots D$:\n",
    "    1. Sample the topic weights $\\theta_d$ from a Dirichlet distribution.\n",
    "    2. For each word-position $n=1\\ldots N$:\n",
    "      1. Sample a topic $z_{d,n}$.\n",
    "      2. Sample a word $w_{d,n}$ from topic $z_{d,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA implementation\n",
    "- Input X is the word counts from `CountVectorizer`.\n",
    "- Important parameters:\n",
    "  - `n_components` - The number of topics ($K$).\n",
    "  - `doc_topic_prior` - Smoothing parameter ($\\alpha$) for the topic weights $\\theta$.\n",
    "  - `topic_word_prior` - Smoothing parameter ($\\eta$) for the topic vector $\\beta$.\n",
    "- Note: in the sklearn implementation,\n",
    "  - the topic weights are not normalized on output.\n",
    "  - the topic vectors are not normalized in `components_`\n",
    "  - can be parallelized (`n_jobs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit LDA model\n",
    "lda = decomposition.LatentDirichletAllocation(\n",
    "    n_components=5,\n",
    "    doc_topic_prior=0.01,\n",
    "    topic_word_prior=0.01,\n",
    "    random_state=4487, max_iter=25, n_jobs=-1)\n",
    "# fit with X\n",
    "Wlda = lda.fit_transform(X)\n",
    "\n",
    "# normalize rows to be probabilities\n",
    "Wlda /= sum(Wlda, axis=1)[:,newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic vectors\n",
    "  - Note that there is a small probability for each word (controlled by smoothing parameter $\\eta$).\n",
    "    \n",
    "    \n",
    "- Document vector\n",
    "  - Note the small probability for each topic (controlled by smoothing parameter $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Linear Dimensionality Reduction - Summary\n",
    "- **Goal:** given set of input vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$, with $\\mathbf{x}_i\\in\\mathbb{R}^d$, represent each input vector as lower-dimensional vector $\\mathbf{w}_i \\in \\mathbb{R}^p$.\n",
    "  - Approximate $\\mathbf{x}$ as a weighted sum of basis vectors $\\mathbf{v}_j\\in \\mathbb{R}^d$\n",
    "    - $\\hat{x} = \\sum_{j=1}^p w_j \\mathbf{v}_j$\n",
    "    - minimize the reconstruction error of $\\hat{\\mathbf{x}}$.\n",
    "  - enables faster processing, or reduces noise.\n",
    "  \n",
    "<table style=\"font-size:9pt;\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Objective</th>\n",
    "<th>Advantages</th>\n",
    "<th>Disadvantages</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Principal component analysis (PCA)</td>\n",
    "<td>minimize reconstruction error; preserve the most variance of data</td>\n",
    "<td>- captures correlated dimensions, removes redundant dimensions, removes noise.<br>- closed-form solution</td>\n",
    "<td>- does not consider end goal (e.g., classification)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Random Projections</td>\n",
    "<td>sample random basis vectors.</td>\n",
    "<td>- fast.<br>- preserves pairwise distances between points (up to accuracy factor).</td>\n",
    "<td>- adds noise to the pairwise distances.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Fisher's Linear Discriminant (FLD)</td>\n",
    "<td>maximize class separation</td>\n",
    "<td>- preserves class separation</td>\n",
    "<td>- requires class information</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Latent Semantic Analysis (LSA)</td>\n",
    "<td>minimize reconstruction error</td>\n",
    "<td>- topic vectors have semantic meaning (co-occuring words)<br>- closed-form solution</td>\n",
    "<td>- topic weights and topic vectors can be negative<br>- does not consider end goal (e.g., classification)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Non-negative Matrix Factorization (NMF)</td>\n",
    "<td>minimize reconstruction error; non-negative weights and basis vectors.</td>\n",
    "<td>- \"additive\" topic/parts model for text or images<br>- sparse topic weights.</td>\n",
    "<td>- solution requires iterative algorithm.<br>- does not consider end goal (e.g., classification)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Latent Dirichlet Allocation (LDA)</td>\n",
    "<td>document is a mixture of topics.</td>\n",
    "<td>- generative probabilistic model.<br>- robust when dataset size is small.</td>\n",
    "<td>- inference/training can be slow for larger datasets.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other things\n",
    "- _Feature Normalization_\n",
    "  - PCA and LDA are based on the covariance between input dimensions.\n",
    "  - applying _per-feature_ normalization will yield a different PCA result!\n",
    "    - normalizing each input dimension changes the relative covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Non-Linear Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA\n",
    "- _How to project to a non-linear surface?_\n",
    "  - apply a high-dimensional feature transformation to the data\n",
    "    - $\\mathbf{x}_i \\Rightarrow \\phi(\\mathbf{x}_i)$\n",
    "  - project high-dim data to a linear surface\n",
    "    - i.e. run PCA on $\\phi(\\mathbf{x}_i)$\n",
    "  - in the original space, the projection will be non-linear\n",
    "<img src=\"imgs/kpca.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel principal components\n",
    "- kernel principal component $\\mathbf{v}$ is a linear combination of high-dim vectors\n",
    "  - $\\mathbf{v} = \\sum_{i=1}^n \\alpha_i \\phi(\\mathbf{x}_i)$\n",
    "  - where $\\alpha_i$ are learned weights.\n",
    "- For a new point $\\mathbf{x}_*$, the KPCA coefficient for $\\mathbf{v}$ is\n",
    "  - $w = \\phi(\\mathbf{x}_*)^T\\mathbf{v} = \\sum_{i=1}^n \\alpha_i\\phi(\\mathbf{x}_*)^T \\phi(\\mathbf{x}_i) = \\sum_{i=1}^n \\alpha_i k(\\mathbf{x}_*, \\mathbf{x}_i)$\n",
    "  - coefficient is based on similarity to data points belonging to $\\mathbf{v}$.\n",
    "  - using the kernel trick saves computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run KPCA\n",
    "kpca = decomposition.KernelPCA(n_components=1, kernel='poly', gamma=0.15, degree=2, coef0=0, n_jobs=-1)\n",
    "W = kpca.fit_transform(X)\n",
    "testW = kpca.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/KPCA-1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF kernel\n",
    "- principal components separate the data into clusters\n",
    "- coefficient is distance to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run KPCA\n",
    "kpca = decomposition.KernelPCA(n_components=8, kernel='rbf', gamma=0.15, n_jobs=-1)\n",
    "W = kpca.fit_transform(X)\n",
    "testW = kpca.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/KPCA-2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPCA Summary\n",
    "- Use kernel trick to perform PCA in high-dimensional space.\n",
    "  - Coefficients are based on a non-linear projection of the data.\n",
    "  - The type of projection is based on the kernel function selected.\n",
    "- Using RBF kernel, KPCA can split the data into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Manifold Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Embedding\n",
    "- Reduce high-dimensional data to 2 or 3 dimensions for visualization\n",
    "- Try to preserve the inherent structure of the data\n",
    "  - find a set of lower-dim points that optimize some criteria.\n",
    "- Two types:\n",
    "  - 1) preserve local neighborhood structure\n",
    "    - assumes thata lies in a lower-dim manifold; unfold the manifold\n",
    "  - 2) preserve pairwise distances (similarities) between points\n",
    "<img src=\"imgs/manifold.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Locally-linear Embedding (LLE)\n",
    "- **Idea:** preserve linearity within local neighborhoods defined by $K$ nearest neighbors.\n",
    "  - 1) a point $\\mathbf{x}_i$ can be reconstructed by a linear combination of its neighbors $N_i$.\n",
    "    - find the weights for the best reconstruction\n",
    "      - $W^* = \\mathop{\\mathrm{argmin}}_W \\sum_i \\|\\mathbf{x}_i - \\sum_{j\\in N_i}w_{i,j}\\mathbf{x}_j\\|^2 $\n",
    "  - 2) the embedded point $\\mathbf{y}_i$ should also have the same local linearity.\n",
    "    - find the embedded points that best preserve the linearity\n",
    "      - $Y^* = \\mathop{\\mathrm{argmin}}_Y \\sum_i \\| \\mathbf{y}_i - \\sum_{j\\in N_i}w_{i,j} \\mathbf{y}_j\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/lle.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors = number of nearest neighbors to use for local neighborhood\n",
    "# n_components = number of dimensions of manifold embedding\n",
    "lle = manifold.LocallyLinearEmbedding(n_neighbors=12, n_components=2, random_state=121, n_jobs=-1)\n",
    "Xr = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/LLE-2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variants of LLE\n",
    "- **Hessian LLE (HLLE)**: LLE that also uses local curvature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set method to 'hessian'\n",
    "hlle = manifold.LocallyLinearEmbedding(method='hessian', n_neighbors=12, n_components=2, random_state=121, n_jobs=-1)\n",
    "Xr = hlle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Local tangent space alignment (LTSA)**: rather than preserve distances, align local tangent spaces of the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set method to 'ltsa'\n",
    "ltsa = manifold.LocallyLinearEmbedding(method='ltsa', n_neighbors=12, n_components=2, random_state=121, n_jobs=-1)\n",
    "Xr = ltsa.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Multidimensional Scaling\n",
    "- **Idea:** find a low-dimensional embedding that preserves the pairwise distances between points in original high-dim space.\n",
    "  - $Y^* = \\mathop{\\mathrm{argmin}}_Y \\sum_{i,j} (d(\\mathbf{x}_i,\\mathbf{x}_j) - d(\\mathbf{y}_i,\\mathbf{y}_j)))^2$\n",
    "    - $\\mathbf{x}_i$ are points in original space\n",
    "    - $\\mathbf{y}_i$ are points in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = manifold.MDS(n_components=2, random_state=1234, n_jobs=-1)\n",
    "Xr = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/MDS-1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem**\n",
    "  - MDS tries to preserve the Euclidean distance between the points.\n",
    "  - For a manifold, two points may be far away along the manifold (geodesic distance), but close in Euclidean distance.\n",
    "    - dashed line = Euclidean distance\n",
    "    - solid line = Geodesic distance\n",
    "<center><img src=\"imgs/geod.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 Isomap (Isometric Mapping)\n",
    "- Find embedding that preserves geodesic distances between points\n",
    "  - geodesic distance = distances moving on the manifold (figure **A**)\n",
    "- Approximate manifold by forming a graph (figure **B**)\n",
    "  - each data point is a node\n",
    "  - edge is added between nodes if they are $K$-nearest-neighbors.\n",
    "  - calculate geodesic distance between 2 points using shortest-paths algorithm (Dijkstra’s algorithm).\n",
    "- Use MDS on the geodesic distances to calculate the embedding (figure **C**).\n",
    "<center><img src=\"imgs/isomap1.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = manifold.Isomap(n_neighbors=12, n_components=2, n_jobs=-1)\n",
    "Xr = iso.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/ISO-1.png\", WIDTH =\"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4 Spectral Embedding (Laplacian Eigenmaps)\n",
    "- Preserve the neighborhood structure\n",
    "  - 1) form an affinity (adjacency) matrix $W$\n",
    "    - using $K$ nearest neighbors\n",
    "      - $W_{i,j} = \\begin{cases} 1, & \\text{$\\mathbf{x}_i$ and $\\mathbf{x}_j$ are neighbors} \\\\ 0, & \\text{they are not neighbors}\\end{cases}$.\n",
    "    - using RBF kernel\n",
    "      - $k(\\mathbf{x}_i, \\mathbf{x}_j)$ represents how close a neighbor.\n",
    "  - 2) keep neighboring points close together in the embedding.\n",
    "    - penalize embedding points that are far apart, but should be neighbors\n",
    "      - $Y^* = \\mathop{\\mathrm{argmin}}_Y \\sum_{i,j} W_{i,j} (y_i - y_j)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nearest-neighbors for affinity\n",
    "spe = manifold.SpectralEmbedding(n_components=2, affinity='nearest_neighbors', \n",
    "                                 random_state=1234, n_neighbors=12, n_jobs=-1)\n",
    "Xr = spe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/SPE.png\", WIDTH =\"500px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RBF kernel for affinity\n",
    "# gamma is the inverse bandwidth\n",
    "spe = manifold.SpectralEmbedding(n_components=2, affinity='rbf', \n",
    "                                 gamma=0.2, random_state=1234)\n",
    "Xr = spe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.5 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Preserve pairwise similarities\n",
    "  - 1) treat similarities in original space as probabilities\n",
    "    - the probability that $j$ is a neighbor of $i$: $p_{j|i} = \\frac{k_i(\\mathbf{x}_i,\\mathbf{x_j})}{\\sum_{k\\neq i} k_i(\\mathbf{x}_i, \\mathbf{x}_k)}$\n",
    "      - $k_i(\\mathbf{x}_i,\\mathbf{x}_j)$ is the RBF kernel with inverse bandwidth $\\gamma_i$.\n",
    "    - the similarity between $i$ and $j$: $p_{i,j} = \\frac{p_{j|i} + p_{i|j}}{2N}$\n",
    "  - 2) similarities in the embedding space (student-t distribution)\n",
    "    - the probability that $i$ and $j$ are neighbors: $q_{i,j} = \\frac{(1 + \\|\\mathbf{y}_i - \\mathbf{y_j}\\|^2)^{-1}}{\\sum_{k\\neq l}(1 + \\|\\mathbf{y}_l - \\mathbf{y_k}\\|^2)^{-1}}$\n",
    "  - 3) find the embedding points that preserve the probability structure:\n",
    "    - $Y = \\mathop{\\mathrm{argmin}}_Y \\sum_{i\\neq j} p_{i,j} \\log \\frac{p_{i,j}}{q_{i,j}}$\n",
    "    - the objective is the Kullback-Leibler (KL) divergence, which is a measure of similarity between two probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tends to group together similar items\n",
    "  - student-t distribution is heavy-tailed\n",
    "  - \"moderate\" distances in original space are converted to \"large\" distances in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity = similar to number of neighbors\n",
    "tsne = manifold.TSNE(n_components=2, perplexity=30.0, random_state=11)\n",
    "Xr = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/tSNE.png\", WIDTH =\"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `perplexity` parameter controls the number of groups (size of groups).\n",
    "  - small perplexity: form more groups\n",
    "  - large perplexity: form less groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Non-Linear Dimensionality Reduction - Summary\n",
    "- **Goal:** given high-dim data, find a low-dim representation.\n",
    "  - try to preserve inherent structure of data\n",
    "- Two types of approaches:\n",
    "  - _Non-linear dimensionality reduction_\n",
    "    - calculate low-dim coefficients using non-linear projections (kernel).\n",
    "  - _Manifold embedding_\n",
    "    - Optimize over the embedding (low-dim) points to preserve some property from the high-dim points.\n",
    "    - After training, manifold embedding methods cannot transform a novel (new) point.\n",
    "  \n",
    "<table style=\"font-size:9pt;\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Objective</th>\n",
    "<th>Advantages</th>\n",
    "<th>Disadvantages</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Kernel principal component analysis (KPCA)</td>\n",
    "<td>PCA in high-dim feature space (kernel trick)</td>\n",
    "<td>- can transform novel data</td>\n",
    "<td>- need to calculate kernel matrix;<br>- need to keep training data around for embedding new points.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Locally-Linear Embedding (LLE)</td>\n",
    "<td>preserve local neighborhood distances</td>\n",
    "<td>- good for single low-dim manifold</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Hessian LLE</td>\n",
    "<td>w/ local curvature information.</td>\n",
    "<td>- good for single low-dim manifold</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Local tangent space alignment (LTSA)</td>\n",
    "<td>align tangent spaces</td>\n",
    "<td>- good for single low-dim manifold</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Multi-dimensional scaling (MDS)</td>\n",
    "<td>preserve pairwise distances</td>\n",
    "<td>- good for viewing the space.</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Isometric mapping (Isomap)</td>\n",
    "<td>preserve geodesic distances</td>\n",
    "<td>- good for single low-dim manifold</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Spectral embedding (SE)</td>\n",
    "<td>\"PCA\" on graph representation</td>\n",
    "<td>- good for single low-dim manifold</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>t-distributed Stochastic Neighbor Embedding (t-SNE)</td>\n",
    "<td>preserve pairwise similarities</td>\n",
    "<td>- can discover similar items</td>\n",
    "<td>- cannot embed a novel point.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original idea\n",
    "- _Perceptron_ \n",
    "  - Warren McCulloch and Walter Pitts (1943), Rosenblatt (1957)\n",
    "  - Simulate a neuron in the brain \n",
    "    - 1) take binary inputs (input from nearby neurons)\n",
    "    - 2) multiply by weights (synapses, dendrites)\n",
    "    - 3) sum and threshold to get binary output (output axon)\n",
    "  - Train weights from data.\n",
    "<center><table border=0><tr><td><img src=\"imgs/neuron.png\" width=350></td><td><img src=\"imgs/neuron_model.jpeg\" width=350></tr></table></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Multi-layer Perceptron\n",
    "- Add _hidden_ layers between input and output neurons\n",
    "  - each layer extracts some features from the previous layers\n",
    "  - can represent complex non-linear functions \n",
    "  - train weights using _backpropagation_ algorithm. (1970-80s)\n",
    "  - (now called a _neural network_)\n",
    "<center><table border=0><tr><td><img src=\"imgs/neural_net.jpeg\" width=350></td><td><img src=\"imgs/neural_net2.jpeg\" width=350></tr></table></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Problem:**\n",
    "  - difficult to train.\n",
    "  - sensitive to initialization.\n",
    "  - computationally expensive (at the time).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formally,\n",
    "  - $y = f( \\sum_{j=0}^d w_j x_j) = f(\\mathbf{w}^T\\mathbf{x})$\n",
    "  - $\\mathbf{w}$ is the weight vector.\n",
    "  - $f(a)$ is the activation function\n",
    "    - $f(a) = \\begin{cases}1, & a>0 \\\\ 0, & \\text{otherwise}\\end{cases}$\n",
    "<center><img src=\"imgs/Perceptron.jpg\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron training criteria\n",
    "- Train the perceptron on data $D = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$\n",
    "- Only look at the points that are misclassified.\n",
    "  - Loss is based on how badly misclassified\n",
    "  - $E(\\mathbf{w}) = \\sum_{i=1}^N \\begin{cases} - y_i \\mathbf{w}^T \\mathbf{x}_i, & \\mathbf{x}_i\\text{ is misclassified} \\\\ 0, & \\text{otherwise}\\end{cases}$\n",
    "- Minimize the loss: $\\mathbf{w}^* = \\mathop{\\mathrm{argmin}}_{\\mathbf{w}} E(\\mathbf{w})$\n",
    "    \n",
    "    \n",
    "#### Training algorithm\n",
    "- Computers were slow back then...only look at one data point at a time and use gradient descent.\n",
    "- **Perceptron Algorithm**\n",
    "  - For each point $\\mathbf{x}_i$, \n",
    "    - If the point $\\mathbf{x}_i$ is misclassified,\n",
    "      - Update weights: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i$\n",
    "  - Repeat until no more points are misclassified    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Algorithm\n",
    "- Fails to converge if data is not linearly separable\n",
    "- Rosenblatt proved that the algorithm will converge if the data is linearly separable.\n",
    "  - the number of iterations is inversely proportional to the separation (margin) between classes.\n",
    "  - _This was one of the first machine learning results!_\n",
    "- Different initializations can yield different weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron\n",
    "- Add hidden layers between the inputs and outputs\n",
    "  - each hidden node is a Perceptron (with its own set of weights)\n",
    "    - its inputs are the outputs from previous layer\n",
    "    - extracts a feature pattern from the previous layer\n",
    "  - can model more complex functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formally, for one layer: \n",
    "  - $\\mathbf{h} = f(\\mathbf{W}^T \\mathbf{x})$\n",
    "    - Weight matrix $\\mathbf{W}$ - one column for each node\n",
    "    - Input $\\mathbf{x}$ - from previous layer\n",
    "    - Output $\\mathbf{h}$ - to next layer\n",
    "    - $f(a)$ is the activation function - applied to each dimension to get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "- There are different types of activation functions:\n",
    "  - _Sigmoid_ - output [0,1]\n",
    "  - _Tanh_ - output [-1,1]\n",
    "  - _Rectifier Linear Unit (ReLU)_ - output [0,$\\infty$]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/ACT-1.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation functions specifically for output nodes:\n",
    "  - _Linear_ - output for regression\n",
    "  - _Softmax_ - output for classification (same as multi-class logistic regression)\n",
    "  \n",
    "- Each layer can use a different activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which activation function is best?\n",
    "- In the early days, only the Sigmoid and Tanh activation functions were used.\n",
    "  - these were notoriously hard to train with.\n",
    "    - \"vanishing gradient\" problem\n",
    "- Recently, ReLU has become very popular.\n",
    "  - easier to train with - no \"vanishing gradient\"\n",
    "  - faster - don't need to calculate exponential\n",
    "  - sparse representation - most nodes will output zero.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an MLP\n",
    "- For classification, we use the cross-entropy loss\n",
    "  - $E = -\\sum_{j=1}^K y_j \\log \\hat{y}_j$\n",
    "    - $y_j$ is 1 for the true class, and 0 otherwise\n",
    "    - $\\hat{y}_j$ is the softmax output for the j-th class\n",
    "- Use gradient descent as before:\n",
    "  - $w_{ij} \\leftarrow w_{ij} - \\eta \\frac{dE}{dw_{ij}}$\n",
    "    - layer $i$, node $j$\n",
    "  - $\\eta$ is the learning rate\n",
    "    - controls convergence rate\n",
    "      - too small --> converges very slowly\n",
    "      - too large --> possibly doesn't converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation (backward propagation)\n",
    "- Do a forward pass to calculate the prediction\n",
    "- Do a backward pass to update weights that were responsible for an error\n",
    "<center><img src=\"imgs/BP.png\" width=\"70%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent with the chain-rule\n",
    "- Suppose we have a 2-layer network\n",
    "  - $E$ is the cost function\n",
    "  - $g_1$, $g_2$ are the output functions of the two layers\n",
    "    - $g_j(\\mathbf{x}) = f(\\mathbf{W}_j^T\\mathbf{x})$\n",
    "    - $\\mathbf{W}_1$, $\\mathbf{W}_2$ are the weight matrices\n",
    "- Prediction for input $\\mathbf{x}$: $y = g_2(g_1(\\mathbf{x}))$\n",
    "- Cost for input $\\mathbf{x}$: $E(\\mathbf{x}) = E(g_2(g_1(\\mathbf{x})))$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Apply the chain rule to get the gradients of weights in layer\n",
    "  - $\\frac{dE(\\mathbf{x})}{d\\mathbf{W}_2} = \\frac{dE}{dg_2}\\frac{dg_2}{d\\mathbf{W}_2}$\n",
    "  - $\\frac{dE(\\mathbf{x})}{d\\mathbf{W}_1} = \\frac{dE}{dg_2}\\frac{dg_2}{dg_1}\\frac{dg_1}{d\\mathbf{W}_1}$\n",
    "- Defines a set of recursive relationships\n",
    "  - 1) calculate the output of each node from first to last layer\n",
    "  - 2) calculate the gradient of each node from last to first layer\n",
    "- NOTE: the gradients multiply in each layer!\n",
    "  - if two gradients are small (<1), their product will be even smaller.  This is the \"vanishing gradient\" problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "- The datasets needed to train NN are typically very large\n",
    "- Use SGD so that only a small portion of the dataset is needed at a time\n",
    "  - Each small portion is called a _mini-batch_\n",
    "  - Use a _momentum_ term, which averages the current gradient with those from previous mini-batches.\n",
    "  - One complete pass through the data is called an _epoch_.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Other Tricks\n",
    "- Normalize the inputs to [-1,1] or [0,1]\n",
    "  - improves numerical stability.\n",
    "- Separate the training set into training and validation\n",
    "  - use the training set to run backpropagation\n",
    "  - test the NN on the validation set for diagnostics\n",
    "    - check for convergence - adjust learning rate if necessary\n",
    "    - check for diverging loss - adjust learning rate\n",
    "    - stopping criteria - stop when no change in the validation error.\n",
    "    - decay learning rate after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TensorFlow backend\n",
    "%env KERAS_BACKEND=tensorflow     \n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, Flatten, Input\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels to binary indicators (required for Keras)\n",
    "Yb = keras.utils.np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed\n",
    "random.seed(4487); tensorflow.set_random_seed(4487)\n",
    "\n",
    "# build the network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units=20, input_dim=2, activation='relu'))\n",
    "nn.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# compile and fit the network\n",
    "nn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.SGD(lr=0.3, momentum=0.9, nesterov=True))\n",
    "history = nn.fit(X, Yb, epochs=100, batch_size=32, validation_split=0.1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = nn.predict_classes(testX, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "- Continuous training will sometimes lead to overfitting\n",
    "  - the training loss decreases, but the validation loss increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping\n",
    "\n",
    "- Training can stopped when the validation loss is stable for a number of iterations\n",
    "  - stable means change below a threshold\n",
    "  - this is to prevent overfitting the training data.\n",
    "  - we can limit the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup early stopping callback function\n",
    "earlystop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',     # look at the validation loss\n",
    "    min_delta=0.0001,       # threshold to consider as no change\n",
    "    patience=5,             # stop if 5 epochs with no change\n",
    "    verbose=1, mode='auto'\n",
    ")\n",
    "callbacks_list = [earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal Approximation Theorem\n",
    "- Cybenko (1989), Hornik (1991)\n",
    "  - _A multi-layer perceptron with a single hidden layer and a finite number of nodes can approximate any continuous function._\n",
    "    - The number of nodes needed might be very large.\n",
    "    - Doesn't say anything about how difficult it is to train it.\n",
    "- Deep learning corrolary\n",
    "  - A deep network can learn the same function using less nodes.\n",
    "  - Given the same number of nodes, a deep network can learn more complex functions.\n",
    "    - Doesn't say anything about how difficult it is to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = nn.get_layer(index=0).get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Inputs and Neural Networks\n",
    "- In the MLP, each node takes inputs from all other nodes in the previous layer. \n",
    "  - For image input, we transform the image into a vector, which is the input into the MLP.\n",
    "    \n",
    "<center>\n",
    "<table><tr>\n",
    "<td><img src=\"imgs/ducklings.jpg\" width=300></td>\n",
    "<td><img src=\"imgs/2d1d.svg\" width=200></td>\n",
    "<td><img src=\"imgs/neural_net.jpeg\" width=300></td>\n",
    "</tr></table></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network (CNN)\n",
    "- Use the spatial structure of the image\n",
    "- 2D convolution filter\n",
    "  - the weights $\\mathbf{W}$ form a 2D filter template\n",
    "  - filter response: $h = f(\\sum_{x,y} W_{x,y} P_{x,y})$\n",
    "    - $\\mathbf{P}$ is an image patch with the same size as $\\mathbf{W}$.\n",
    "- Convolution feature map\n",
    "  - pass a sliding window over the image, and apply filter to get a _feature map_.\n",
    "<center><img src=\"imgs/Convolution_schematic.gif\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolution modes\n",
    "  - **\"valid\"** mode - only compute feature where convolution filter has valid image values.\n",
    "    - size of feature map is reduced.\n",
    "<center><img src=\"imgs/conv-valid.gif\" width=400></center>\n",
    "\n",
    "\n",
    "\n",
    "- Convolution modes\n",
    "  - **\"same\"** mode - zero-pad the border of the image\n",
    "    - feature map is the same size as the input image.\n",
    "<center><img src=\"imgs/conv-same.gif\" width=400></center>\n",
    "\n",
    "\n",
    "- `(Usually \"same\" is better since it looks at structures around border)`\n",
    "\n",
    "\n",
    "- Convolutional layer\n",
    "  - **Input:** HxW image with C channels\n",
    "    - For example, in the first layer, C=3 for RGB channels.\n",
    "    - defines a 3D volume: C x H x W\n",
    "  - **Features:** apply F convolution filters to get F feature maps.\n",
    "    - Uses 3D convolution filters: weight volume is C x K x K\n",
    "    - K is the spatial extent of the filter\n",
    "  - **Output:** a feature map with F channels \n",
    "    - defines a 3D volume: F x H' x W'\n",
    "    \n",
    "<center><img src=\"imgs/depthcol2.jpeg\" width=350></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - Spatial sub-sampling\n",
    "    - reduce the feature map size by subsampling feature maps between convolutional layers\n",
    "      - *stride* for convolution filter - step size when moving the windows across the image. <center><img src=\"imgs/conv-stride.gif\" width=300></center>      \n",
    "      - *max-pooling* - use the maximum over the pooling window\n",
    "        - gathers features together, makes it robust to small changes in configuration of features\n",
    "<center><img src=\"imgs/conv-pool.png\" width=300></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Convolution Layers\n",
    "- The convolutional filters extract the same features throughout the image.\n",
    "  - Good when the object can appear in different locations of the image.\n",
    "- Pooling makes it robust to changes in feature configuration, and translation of the object.\n",
    "- The number of parameters is small compared to Dense (Fully-connected) layer\n",
    "  - Example: input is C x H x W, and output is F x H x W\n",
    "    - Number of MLP parameters: (CHW+1) x (FHW)\n",
    "    - Number of CNN parameters: (CKK+1) x (FKK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.add(Conv2D(10, (5,5), strides=(2,2), activation='relu',\n",
    "              input_shape=(1,28,28),\n",
    "              padding='same', data_format='channels_first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization with \"Dropout\"\n",
    "- During training, randomly \"drop out\" each node with probability $p$\n",
    "  - a dropped-out node is not used for calculating the prediction or weight updating.\n",
    "  - trains a reduced network in each iteration.\n",
    "\n",
    "<center><img src=\"imgs/dropout.jpeg\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During test time, use all the nodes for prediction and scale output by $p$.\n",
    "  - Similar to creating an ensemble of networks, and then averaging the predictions.\n",
    "- Prevents overfitting\n",
    "  - also improves the training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.add(Dropout(rate=0.5, seed=44))   # dropout layer! (need to specify the seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization with Weight Decay\n",
    "- Another way to regularize the network is to use \"weight decay\"\n",
    "  - Add a penalty term to the loss function\n",
    "    - larger weights impose higher penalty\n",
    "    - $L = Loss + \\alpha \\sum_{i} w_i^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.add(Conv2D(80, (5,5), strides=(1,1), activation='relu',\n",
    "              kernel_regularizer=keras.regularizers.l2(0.0001),  # L2 regularizer\n",
    "              padding='same', data_format='channels_first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "- artificially permute the data to increase the dataset size\n",
    "  - goal: make the network invariant to the permutations\n",
    "  - examples: translate image, flip image, add pixel noise, rotate image, deform image, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# build the data augmenter\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,         # image rotation\n",
    "    width_shift_range=0.2,     # image shifting\n",
    "    height_shift_range=0.2,    # image shifting\n",
    "    shear_range=0.1,           # shear transformation\n",
    "    zoom_range=0.1,            # zooming\n",
    "    data_format='channels_first')\n",
    "\n",
    "# fit (required for some normalization augmentations)\n",
    "datagen.fit(vtrainI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data through augmentor and fit\n",
    "# runs data-generator and fit in parallel\n",
    "history = nn.fit_generator(\n",
    "            datagen.flow(vtrainI, vtrainYb, batch_size=50),  # data from generator\n",
    "            steps_per_epoch=len(vtrainI)/50,    # should be number of batches per epoch\n",
    "            epochs=100,\n",
    "            callbacks=callbacks_list, \n",
    "            validation_data=validsetI, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation with noise\n",
    "- Also add per-pixel noise to the image for data augmentation.\n",
    "  - define a function to add noise\n",
    "  - set it as the `preprocessing_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gauss_noise(X, sigma2=0.05):\n",
    "    # add Gaussian noise with zero mean, and variance sigma2\n",
    "    return X + random.normal(0, sigma2, X.shape)\n",
    "\n",
    "# build the data augmenter\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,         # image rotation\n",
    "    width_shift_range=0.2,     # image shifting\n",
    "    height_shift_range=0.2,    # image shifting\n",
    "    shear_range=0.1,           # shear transformation\n",
    "    zoom_range=0.1,            # zooming\n",
    "    preprocessing_function=add_gauss_noise, \n",
    "    data_format='channels_first')\n",
    "\n",
    "# fit (required for some normalization augmentations)\n",
    "datagen.fit(vtrainI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- **Different types of neural networks**\n",
    "  - _Perceptron_ - single node (similar to logistic regression)\n",
    "  - _Multi-layer perceptron (MLP)_ - collection of perceptrons in layers\n",
    "    - also called _Fully-connected layer_\n",
    "  - _Convolutional neural network (CNN)_ - convolution filters for extracting local image features\n",
    "- **Training**\n",
    "  - optimize loss function using stochastic gradient descent\n",
    "- **Advantages**\n",
    "  - lots of parameters - large capacity to learn from large amounts of data\n",
    "- **Disadvantages**\n",
    "  - lots of parameters - easy to overfit data\n",
    "    - need to regularize parameters (dropout, L2)\n",
    "    - need to monitor the training process\n",
    "  - sensitive to initialization, learning rate, training algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Image Classification and Deep Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Classifiers in Keras\n",
    "- Keras includes several pre-trained image classifier networks\n",
    "  - VGG16, VGG19, ResNet50, InceptionV3, ...\n",
    "  - already trained on ImageNet\n",
    "- Can use these networks to:\n",
    "  - perform image classification (for 1000 classes only)\n",
    "  - extract image features for our own task\n",
    "  - transfer learning - modify/adapt a pre-trained network for our own task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TensorFlow backend\n",
    "%env KERAS_BACKEND=tensorflow     \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, Flatten, \\\n",
    "                Input, MaxPooling2D, UpSampling2D, Lambda, Reshape, BatchNormalization, \\\n",
    "                GlobalAveragePooling2D\n",
    "import keras\n",
    "import tensorflow\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "import struct\n",
    "\n",
    "# use channels first representation for images\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Learning\n",
    "- The network is learning a function (image to class)\n",
    "  - build the function block-by-block\n",
    "  - each block learns a residual, which is added to the previous block\n",
    "    - keep all the previous information and make small changes with the residual\n",
    "<center><img src=\"imgs/ResNet-Block.png\" width=300></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of the ResNet50 model\n",
    "  - trained on ImageNet\n",
    "  - will download the model if loading for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains the model and support functions for ResNet50\n",
    "import keras.applications.resnet50 as resnet\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "# create an instance of the model\n",
    "model = resnet.ResNet50(weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Image Features\n",
    "- Use the pre-trained network as a feature extractor\n",
    "  - remove the last layer (the classifier)\n",
    "  - apply average pooling on the feature map\n",
    "    - 7x7x2048 --> 1x2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model w/o the last layer\n",
    "model_f = resnet.ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning and fine-tuning\n",
    "- The network can also be \"fine-tuned\" for a new image classification task.\n",
    "  - This is called transfer learning.\n",
    "- Rather than retrain the whole network, \n",
    "  - fix the lower layers that extract features (no need to train them since they are good features)\n",
    "  - train the last few layers to perform the new task\n",
    "- Does not need as much data, compared to the original ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4487); tensorflow.set_random_seed(4487)\n",
    "\n",
    "# create the base pre-trained model with-out the classifier\n",
    "base_model = resnet.ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# start with the output of the ResNet50 (7x7x2048)\n",
    "x = base_model.output\n",
    "\n",
    "# for each channel, average all the features (1x2048)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# fully-connected layer (1 x32)\n",
    "# (only two classes so don't need so many)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "# finally, the softmax for the classifier (2 classes)\n",
    "predictions = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Model\n",
    "  - specify the input and output layers\n",
    "  - the network is everything in between\n",
    "- Fix the weights of the layers of ResNet so they will not change during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for training\n",
    "# - need to specify the input layer and the output layer\n",
    "model_ft = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# fix the layers of the ResNet50.\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model - only the layers that we added will be trained\n",
    "model_ft.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def add_gauss_noise(X, sigma2=0.05):\n",
    "    # add Gaussian noise with zero mean, and variance sigma2\n",
    "    return X + random.normal(0, sigma2, X.shape)\n",
    "\n",
    "# build the data augmenter\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,         # image rotation\n",
    "    width_shift_range=0.2,     # image shifting\n",
    "    height_shift_range=0.2,    # image shifting\n",
    "    shear_range=0.1,           # shear transformation\n",
    "    zoom_range=0.1,            # zooming\n",
    "    horizontal_flip=True, \n",
    "    preprocessing_function=add_gauss_noise, \n",
    ")\n",
    "\n",
    "# fit (required for some normalization augmentations)\n",
    "datagen.fit(vtrainXim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "bsize = 32\n",
    "callbacks_list = []\n",
    "history = model_ft.fit_generator(\n",
    "            datagen.flow(vtrainXim, vtrainYb, batch_size=bsize),  # data from generator\n",
    "            steps_per_epoch=len(vtrainXim)/bsize,    # should be number of batches per epoch\n",
    "            epochs=20,\n",
    "            callbacks=callbacks_list, \n",
    "            validation_data=validset, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predYscore = model_ft.predict(testXim, verbose=False)\n",
    "predY = argmax(predYscore, axis=1)\n",
    "acc = metrics.accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Neural Networks and Unsupervised Learning\n",
    "- How to use NN for dimensionality reduction or clustering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising Autoencoder\n",
    "- Use the hidden layer as the lower-dimensional representation (code)\n",
    "- Train the network to \"encode\" and \"decode\"\n",
    "  - randomly corrupt the input (by setting values to 0)\n",
    "  - run it through the encoding-decoding network\n",
    "  - minimize the difference between the output and the original input\n",
    "<center><img src=\"imgs/ae.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the autoencoder\n",
    "  - specify the number of hidden nodes\n",
    "  - corrupt the image using Dropout\n",
    "    - corruption level = percentage of inputs that are zeroed out.\n",
    "\n",
    "\n",
    "\n",
    "- Use `Model` class.\n",
    "  - pass input and output layers. \n",
    "  - The model consists of everything between input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed\n",
    "random.seed(4487); tensorflow.set_random_seed(4487)\n",
    "\n",
    "# Build the Encoder model\n",
    "input_img = Input(shape=(784,))\n",
    "corrupted_img = Dropout(rate=0.3)(input_img)\n",
    "encoded = Dense(10, activation='relu')(corrupted_img)\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Build the Decoder model\n",
    "encoded_input = Input(shape=(10,))\n",
    "decoded = Dense(784, activation='sigmoid')(encoded_input)\n",
    "decoder = Model(encoded_input, decoded)\n",
    "\n",
    "# build the full autoencoder model\n",
    "autoencoder = Model(input_img, decoder(encoder(input_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping criteria\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                            min_delta=0.0001, patience=10,\n",
    "                            verbose=1, mode='auto')\n",
    "                \n",
    "# compile and fit the network\n",
    "autoencoder.compile(loss=keras.losses.binary_crossentropy,\n",
    "           optimizer=keras.optimizers.SGD(lr=0.2, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model: the input and output are the same\n",
    "# write to a log directory to see training process\n",
    "history = autoencoder.fit(vtrainXraw, vtrainXraw, \n",
    "                 epochs=20, batch_size=50, \n",
    "                 callbacks=[earlystop, TensorBoard(log_dir='./logs/ae')],\n",
    "                 validation_data=(validXraw, validXraw), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run tensorboard in console: `tensorboard --logdir=./logs.ae`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder.predict(trainXraw)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the reconstruction of the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testXrecon = decoder.predict(encoder.predict(testXraw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational AutoEncoder (VAE)\n",
    "- The standard autoencoder can have difficulty encoding/decoding new images\n",
    "  - the decoder never sees (encoded) latent vectors outside of the training set\n",
    "- VAE fixes this by introducing noise in the latent vectors\n",
    "  - the noise lets the decoder network see slightly different latent vectors for each training image.\n",
    "  - improves the ability to interpolate between training samples\n",
    "  \n",
    "<center><img src=\"imgs/vae2.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build the encoder\n",
    "- Map the input into the mean and log(sigma) of the Gaussian distribution\n",
    "  - the mean is the encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder mapping to distribution (mean and log_sigma)\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "# the man and log-sigma\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, z_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the mean and log(sigma) to sample a latent variable z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling function - draw Gaussian random noise\n",
    "epsilon_std = 0.001\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# layer that samples according to mean and sigma\n",
    "z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decode the latent variable z\n",
    "- Construct the whole VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the layers and assign to a variable, since we need \n",
    "# to use it later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "\n",
    "# connect the latent variable and hidden states\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# end-to-end variational autoencoder\n",
    "vae = Model(x, x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construxct the generator (decoder)\n",
    "  - attach another input to the saved layers, and connect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))  # make an input and attach it to the hidden state layer\n",
    "_h_decoded = decoder_h(decoder_input)       # and other layers\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "\n",
    "# the generator model\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VAE uses a special loss function\n",
    "  - minimize the KL divergence between the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the VAE loss\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # cross-entropy loss\n",
    "    xent_loss = keras.losses.binary_crossentropy(x, x_decoded_mean)\n",
    "    # KL divergence loss\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "# compile the model for optimization\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(vtrainXraw, vtrainXraw,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(validXraw, validXraw),        \n",
    "            callbacks=[TensorBoard(log_dir='./logs/vae')],\n",
    "            verbose=False\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- **Deep architectures**\n",
    "  - advances of deep learning has been driven by the ImageNet competiton.\n",
    "  - error rate decreases as the depth increases.\n",
    "  - as depth increases, need to have a smart architecture design to make training more effective.\n",
    "- **Unsupervised Learning**\n",
    "  - Autoencoder - unsupervised dimensionality reduction and clustering.\n",
    "  - Convolutional autoencoder - AE for images.\n",
    "  - Variational autoencoder - improve interpolation ability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
